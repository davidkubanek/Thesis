{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkubanek/Thesis/blob/main/model_experiments_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpnAQeFgIIKA"
      },
      "source": [
        "# CONCERTO architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1jm2t5MI4af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95217f9-102c-4d0d-c504-db9dc794f54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-1.1.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.2\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.2\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=3cfaaed4a1db52794111ddfa5706d2a910c24e3d82a0497dea37b8b31d3e57bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ef14289cec10bb710105d0add6b5f66ca0e254ba1eae7ed559da40aa53e214cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n"
          ]
        }
      ],
      "source": [
        "# for running in colab\n",
        "!pip install dgl\n",
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69lAzjYIIKB",
        "outputId": "9c41c0c5-27e2-4723-a2d9-8bf5dc2b0187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  plt.rcParams[\"font.family\"] = \"Palatino\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.loader import DataLoader\n",
        "import wandb\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "# check if cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQK8GKzgKVqk",
        "outputId": "c30191f4-b1a5-4175-8010-2be1b4827ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "MG-9VYyW7Dud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def prepare_datalist(matrix_df, args, graph_fp=True, grover_fp=None):\n",
        "    '''\n",
        "    Convert matrix dataframe to a data_list with pytorch geometric graph data, fingerprints and labels.\n",
        "    Inputs:\n",
        "        matrix_df: dataframe of SMILES, assays and bioactivity labels\n",
        "        args: arguments\n",
        "        graph_fp: if True, includes graph embedding fingerprints into data_list\n",
        "        grover_fp: if True, includes GROVER graph transformer embedding fingerprints into data_list\n",
        "    Outputs:\n",
        "        data_list: list of data objects\n",
        "    '''\n",
        "    # only use subset of data (assays and data points)\n",
        "    assay_list = args['assay_list']\n",
        "    num_assays = args['num_assays']\n",
        "    assay_start = args['assay_start']\n",
        "    num_data_points = args['num_data_points']\n",
        "\n",
        "    # get binary target labels\n",
        "    y = matrix_df[assay_list[assay_start:assay_start+num_assays]].values[:num_data_points]\n",
        "\n",
        "    # get SMILES strings\n",
        "    data = matrix_df['SMILES'].values[:num_data_points]\n",
        "\n",
        "    if graph_fp is True: # add graph fingerprint\n",
        "        GraphDataset = GraphDatasetClass()\n",
        "        # create pytorch geometric graph data list\n",
        "        data_list = GraphDataset.create_pytorch_geometric_graph_data_list_from_smiles_and_labels(data, y)\n",
        "    else: # create simple data_list without graph fingerprint\n",
        "      data_list = []\n",
        "      for label in y:\n",
        "          # construct Pytorch Geometric data object and append to data list\n",
        "          data_list.append(Data(y = label.reshape(1, -1)))\n",
        "\n",
        "    # add fingerprint data to each graph\n",
        "    for i, smile in tqdm(enumerate(data), desc='Adding fingerprints...', total=len(data)):\n",
        "        fp = convert_smile_to_fp_bit_string(smile)\n",
        "        data_list[i].fp = fp\n",
        "\n",
        "\n",
        "    # add grover fingerprint to each graph\n",
        "    if grover_fp is not None:\n",
        "        for i, gfp in tqdm(enumerate(grover_fp['fps'][:args['num_data_points']]), desc='Adding grover embedding...', total=len(data)):\n",
        "          data_list[i].grover_fp = torch.tensor(gfp)\n",
        "\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def prepare_dataloader(data_list, args):\n",
        "    '''\n",
        "    Get dataloader dictionary from data_list with desired batch_size\n",
        "    '''\n",
        "    data_list = data_list[:args['num_data_points']]\n",
        "    # split into train and test\n",
        "    train_dataset = data_list[:int(len(data_list)*0.8)]\n",
        "    test_dataset = data_list[int(len(data_list)*0.8):]\n",
        "\n",
        "    # split into train and validation\n",
        "    val_dataset = train_dataset[:int(len(train_dataset)*0.25)]\n",
        "    train_dataset = train_dataset[int(len(train_dataset)*0.25):]\n",
        "\n",
        "    print(f'Number of training graphs: {len(train_dataset)}')\n",
        "    print(f'Number of validation graphs: {len(val_dataset)}')\n",
        "    print(f'Number of test graphs: {len(test_dataset)}')\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    # create data loaders\n",
        "    dataloader = {}\n",
        "    dataloader['train'] = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "    dataloader['val'] = DataLoader(val_dataset, batch_size=args['batch_size'], shuffle=False)\n",
        "    dataloader['test'] = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def analyze_dataset(dataset, args):\n",
        "    '''\n",
        "    Analyze the distribution of positive classes in the dataset\n",
        "    '''\n",
        "    positive = []\n",
        "    for i in range(len(dataset)):\n",
        "        positive.append(dataset[i].y[0].sum().item())\n",
        "\n",
        "\n",
        "    num_assays = args['num_assays']\n",
        "    # make histogram of the number of positive\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    # define bins\n",
        "    bins = np.linspace(0, num_assays, num_assays+1)-0.5\n",
        "    plt.hist(positive, bins=bins, alpha=0.5, label='train')\n",
        "    num_assays = args['num_assays']\n",
        "    plt.xlabel(f'# of positive hits in target vector (out of {num_assays})')\n",
        "    plt.ylabel('Number of data points')\n",
        "    plt.title('Histogram of positive class distribution')\n",
        "    plt.show()\n",
        "\n",
        "    for i in range(num_assays+1):\n",
        "        print(f'Number of data points with {i} positive targets: ', (np.array(positive) == i).sum(), f'({(np.array(positive) == i).sum()/len(positive)*100:.2f}%)')\n",
        "\n",
        "def data_explore(dataloader):\n",
        "    '''\n",
        "    Explore the data\n",
        "    '''\n",
        "    # check proportion of positive and negative samples across each assay\n",
        "    pos = torch.zeros(args['num_assays'])\n",
        "    for data in dataloader:  # Iterate in batches over the training dataset\n",
        "        # print('inputs:')\n",
        "        # print(' x:', data.x.shape, '| y:',data.y.shape, '| fp:',data.fp.shape, '| grover:', data.grover_fp.shape)\n",
        "        pos += data.y.sum(axis=0)\n",
        "        #  print(data.y.sum(axis=0))\n",
        "    print('# positive samples:', pos)\n",
        "    print(torch.round((pos/len(dataloader.dataset)*100),decimals=2),'% are positive')\n",
        "\n"
      ],
      "metadata": {
        "id": "eR47vh5vf_Se"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "directory = '/content/drive/MyDrive/Thesis/Data/'\n",
        "\n",
        "# Specify the path where you saved the dictionary\n",
        "load_path = directory + 'datalist.pkl'\n",
        "\n",
        "# Load the dictionary using pickle\n",
        "with open(load_path, 'rb') as f:\n",
        "    data_list = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "bFUFuaaP7MeT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "dtbv6Cb4Mes0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define hyperparams to sweep"
      ],
      "metadata": {
        "id": "5eEWrR-aTjYI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWyacYyIIKG"
      },
      "source": [
        "# Models\n",
        "### GCN and GCN_FP\n",
        "- GCN: graph embedding followed by a final classification layer\n",
        "- GCN_FP: graph + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1q43d46hIIKG"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    '''\n",
        "    Define a Graph Convolutional Network (GCN) model architecture.\n",
        "    Can include 'graph' only or 'graph + fingerprints' embedding before final classification layer.\n",
        "    '''\n",
        "    def __init__(self, args):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "\n",
        "        num_node_features = args['num_node_features']\n",
        "        hidden_channels = args['hidden_channels']\n",
        "        num_classes = args['num_assays']\n",
        "        if args['model'] == 'GCN_FP':\n",
        "            fp_dim = args['fp_dim']\n",
        "        else:\n",
        "            fp_dim = 0\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin = Linear(hidden_channels + fp_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, fp=None):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # if also using fingerprints\n",
        "        if fp is not None:\n",
        "            # reshape fp to batch_size x fp_dim\n",
        "            fp = fp.reshape(x.shape[0], -1)\n",
        "            # concatenate graph node embeddings with fingerprint\n",
        "            # print('BEFORE CONCAT x:',x.shape, 'fp:', fp.shape)\n",
        "            x = torch.cat([x, fp], dim=1)\n",
        "            # print('AFTER CONCAT x:',x.shape)\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cENu4SMIIKG"
      },
      "source": [
        "### FP, GROVER and GROVER_FP\n",
        "- FP: fingerprints embedding followed by a final classification layer\n",
        "- GROVER: graph transformer embedding followed by a final classification layer\n",
        "- GROVER_FP: graph transformer + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PXTwqghIIIKG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LinearBlock(nn.Module):\n",
        "\t\"\"\" basic block in an MLP, with dropout and batch norm \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_feats, out_feats, dropout=0.1):\n",
        "\t\tsuper(LinearBlock, self).__init__()\n",
        "\t\tself.linear = nn.Linear(in_feats, out_feats)\n",
        "\t\tself.bn = nn.BatchNorm1d(out_feats)\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t# ReLU activation, batch norm, dropout on layer\n",
        "\t\treturn self.bn(self.dropout(F.relu(self.linear(x))))\n",
        "\n",
        "def construct_mlp(in_dim, out_dim, hidden_dim, hidden_layers, dropout=0.1):\n",
        "\t\"\"\"\n",
        "\tConstructs an MLP with specified dimensions.\n",
        "\t\t- total number of layers = hidden_layers + 1 (the + 1 is for the output linear)\n",
        "\t\t- no activation/batch norm/dropout on output layer\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert hidden_layers >= 1, hidden_layers\n",
        "\tmlp_list = []\n",
        "\tmlp_list.append(LinearBlock(in_dim,hidden_dim,dropout=dropout))\n",
        "\tfor i in range(hidden_layers-1):\n",
        "\t\tmlp_list.append(LinearBlock(hidden_dim,hidden_dim,dropout=dropout))\n",
        "\n",
        "\t# no activation/batch norm/dropout on output layer\n",
        "\tmlp_list.append(nn.Linear(hidden_dim,out_dim))\n",
        "\tmlp = nn.Sequential(*mlp_list)\n",
        "\treturn mlp\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\t'''\n",
        "\tMLP with optional Grover fingerprints.\n",
        "\tCustomizable number of layers, hidden dimensions, and dropout.\n",
        "\t'''\n",
        "\tdef __init__(self, args):\n",
        "\n",
        "\t\tsuper(MLP, self).__init__()\n",
        "\n",
        "\t\tself.model_type = args['model']\n",
        "\t\tself.fp_dim = args['fp_dim'] # can be 0\n",
        "\t\tself.grover_fp_dim = args['grover_fp_dim'] # can be 0\n",
        "\t\tself.hidden_dim = args['hidden_channels']\n",
        "\t\tself.output_dim = args['num_assays']\n",
        "\t\tself.num_layers = args['num_layers']\n",
        "\t\tself.dropout = args['dropout']\n",
        "\n",
        "\t\tassert self.model_type in ['FP','GROVER','GROVER_FP'], f'model type not supported: {self.model_type}'\n",
        "\n",
        "\t\tif self.model_type == 'FP':\n",
        "\t\t\tself.grover_fp_dim = 0\n",
        "\t\telif self.model_type == 'GROVER':\n",
        "\t\t\tself.fp_dim = 0\n",
        "\n",
        "\t\tself.ff_layers = construct_mlp(\n",
        "\t\t\tself.fp_dim + self.grover_fp_dim,\n",
        "\t\t\tself.output_dim,\n",
        "\t\t\tself.hidden_dim,\n",
        "\t\t\tself.num_layers,\n",
        "\t\t\tself.dropout\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\n",
        "\n",
        "\t\tif self.model_type == 'FP': # only fp is used\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(fingerprints)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER': # only grover is used\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(grover_fp)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER_FP': #grover and fp are concatenated\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(torch.cat([fingerprints, grover_fp], dim=1))\n",
        "\n",
        "\n",
        "\t\treturn output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMXuJkTGIIKH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A3SbXTXlIIKH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "\n",
        "class TrainManager:\n",
        "\n",
        "    def __init__(self, dataloader, args, model=None):\n",
        "\n",
        "        self.args = args\n",
        "        self.num_assays = args['num_assays']\n",
        "        self.num_node_features = args['num_node_features']\n",
        "        self.hidden_channels = args['hidden_channels']\n",
        "\n",
        "        if not model:\n",
        "            # initialize model depending on model type\n",
        "            if args['model'] in ['GCN','GCN_FP']:\n",
        "                self.model = GCN(args)\n",
        "            elif args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "              self.model = MLP(args)\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "        self.model.to(args['device'])\n",
        "        print(\"Model is on device:\", next(self.model.parameters()).device)\n",
        "\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args['lr'])\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.curr_epoch = 0\n",
        "\n",
        "        # logging\n",
        "        self.eval_metrics = {}\n",
        "        self.eval_metrics['loss'] = []\n",
        "        self.eval_metrics['acc_train'] = []\n",
        "        self.eval_metrics['acc_test'] = []\n",
        "        self.eval_metrics['auc_train'] = []\n",
        "        self.eval_metrics['auc_test'] = []\n",
        "        self.eval_metrics['precision_train'] = []\n",
        "        self.eval_metrics['precision_test'] = []\n",
        "        self.eval_metrics['recall_train'] = []\n",
        "        self.eval_metrics['recall_test'] = []\n",
        "        self.eval_metrics['f1_train'] = []\n",
        "        self.eval_metrics['f1_test'] = []\n",
        "\n",
        "\n",
        "    def train(self, epochs=100, log=False, wb_log=False):\n",
        "        '''\n",
        "        Train the model for a given number of epochs.\n",
        "        '''\n",
        "\n",
        "        self.wb_log = wb_log\n",
        "\n",
        "        epoch_times = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            self.model.train()\n",
        "            cum_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Iterate in batches over the training dataset\n",
        "            for data in tqdm(self.dataloader['train'], desc=f'Epoch [{self.curr_epoch}/{epochs}]', total=int(len(self.dataloader['train'].dataset)/self.args['batch_size'])):\n",
        "\n",
        "                data = data.to(self.args['device'])\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                # print('data.y:',data.y.shape)\n",
        "                loss = self.criterion(out, data.y)  # Compute the loss. (sigmoid inherent in loss)\n",
        "                loss.backward()  # Derive gradients.\n",
        "                self.optimizer.step()  # Update parameters based on gradients.\n",
        "                self.optimizer.zero_grad()  # Clear gradients.\n",
        "                cum_loss += loss.item()\n",
        "\n",
        "            self.eval_metrics['loss'].append(cum_loss/len(self.dataloader['train']))\n",
        "            if wb_log is True:\n",
        "                wandb.log({'epoch': self.curr_epoch, \"loss\": cum_loss/len(self.dataloader['train'])})\n",
        "\n",
        "            epoch_time = time.time() - start_time\n",
        "            epoch_times.append(epoch_time)\n",
        "\n",
        "            if log:\n",
        "                # evaluate\n",
        "                acc_train, auc_train, precision_train, recall_train, f1_train = self.eval(self.dataloader['train'])\n",
        "                acc_test, auc_test, precision_test, recall_test, f1_test = self.eval(self.dataloader['val'])\n",
        "\n",
        "\n",
        "                self.eval_metrics['acc_train'].append(acc_train)\n",
        "                self.eval_metrics['acc_test'].append(acc_test)\n",
        "                self.eval_metrics['auc_train'].append(auc_train)\n",
        "                self.eval_metrics['auc_test'].append(auc_test)\n",
        "                self.eval_metrics['precision_train'].append(precision_train)\n",
        "                self.eval_metrics['precision_test'].append(precision_test)\n",
        "                self.eval_metrics['recall_train'].append(recall_train)\n",
        "                self.eval_metrics['recall_test'].append(recall_test)\n",
        "                self.eval_metrics['f1_train'].append(f1_train)\n",
        "                self.eval_metrics['f1_test'].append(f1_test)\n",
        "\n",
        "                if wb_log is True:\n",
        "                    wandb.log({'epoch': self.curr_epoch, \"AUC train\": auc_train, \"AUC test\": auc_test, \"F1 train\": f1_train, \"F1 test\": f1_test, \"Precision train\": precision_train, \"Precision test\": precision_test, \"Recall train\": recall_train, \"Recall test\": recall_test})\n",
        "\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f'Epoch: {self.curr_epoch}, Loss: {loss.item():.4f}, Train AUC: {auc_train:.4f}, Test AUC: {auc_test:.4f}')\n",
        "                    print(f'                        Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}')\n",
        "\n",
        "            self.curr_epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "        self.avg_epoch_time = np.mean(epoch_times)\n",
        "        if wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"avg epoch time\": self.avg_epoch_time})\n",
        "\n",
        "    def eval(self, loader):\n",
        "        '''\n",
        "        Evaluate the model on a given dataset (train/val/test).\n",
        "        '''\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # print(\"Model is on device for eval:\", next(exp.model.parameters()).device)\n",
        "\n",
        "        correct = 0\n",
        "\n",
        "        gts = []\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "\n",
        "                data = data.to(self.args['device'])\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # convert out to binary\n",
        "                pred = torch.round(torch.sigmoid(out))\n",
        "                preds.append(torch.round(torch.sigmoid(out)).tolist())\n",
        "                gts.append(data.y.tolist())\n",
        "                # print('pred:', pred)\n",
        "                # print('data.y:', data.y)\n",
        "                # print('data.y eval:',data.y.shape)\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        preds = [b[i] for b in preds for i in range(len(b))]\n",
        "        gts = [b[i] for b in gts for i in range(len(b))]\n",
        "\n",
        "        auc = roc_auc_score(gts, preds)\n",
        "        # Calculate macro-averaged precision, recall, and F1 Score\n",
        "        precision = precision_score(gts, preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(gts, preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(gts, preds, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "        acc = correct / (len(loader.dataset) * self.args['num_assays']) # Derive ratio of correct predictions.\n",
        "\n",
        "        self.eval_time = time.time() - start_time\n",
        "\n",
        "        if self.wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"eval time\": self.eval_time})\n",
        "\n",
        "        return acc, auc, precision, recall, f1\n",
        "\n",
        "\n",
        "\n",
        "    def analyze(self):\n",
        "        '''\n",
        "        Plot the model performance.\n",
        "        '''\n",
        "\n",
        "        # plot side by side\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        ax1.plot(self.eval_metrics['loss'])\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Losses')\n",
        "\n",
        "        ax2.plot(self.eval_metrics['auc_train'], label='train')\n",
        "        ax2.plot(self.eval_metrics['auc_test'], label='test')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('AUC')\n",
        "        ax2.set_title('Area Under Curve')\n",
        "        ax2.legend()\n",
        "        # make main title for the whole plot\n",
        "        if args['model'] in ['GCN', 'GCN_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Node feats: {self.args[\"num_node_features\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        elif args['model'] in ['FP', 'GROVER', 'GROVER_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Num layers: {self.args[\"num_layers\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, folder, filename, save_weights=True, save_logs=True):\n",
        "        print('saving experiment...')\n",
        "\n",
        "        filename += f'_{self.curr_epoch}e'\n",
        "        if save_weights:\n",
        "            torch.save(self.model.state_dict(), os.path.join(folder, filename+'.pt'))\n",
        "\n",
        "        #if save_logs:\n",
        "\n",
        "    def load_model(self, folder, filename):\n",
        "        print('loading model...')\n",
        "        self.model.load_state_dict(torch.load(os.path.join(folder, filename+'.pt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxwqEHtDIIKH"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "args['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# data parameters\n",
        "args['num_data_points'] = 324191 # all=324191 # number of data points to use\n",
        "#args['assay_list'] = cell_based_high_hr #for all: matrix_df.columns.values[1:]\n",
        "args['num_assays'] = 5 # number of assays to use (i.e., no. of output classes)\n",
        "args['assay_start'] = 0 # which assay to start from\n",
        "\n",
        "args['num_node_features'] = 79 # number of node features in graph representation\n",
        "args['grover_fp_dim'] = 5000 #grover_fp['fps'][0].shape[0] # None  # dim of grover fingerprints\n",
        "args['fp_dim'] = 2215 # dim of fingerprints\n",
        "\n",
        "\n",
        "# training parameters\n",
        "args['model'] = 'GROVER_FP' # 'GCN', 'GCN_FP', 'FP', 'GROVER', 'GROVER_FP'\n",
        "args['num_layers'] = 5 # number of layers in MLP\n",
        "args['hidden_channels'] = 512 # 64\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['num_epochs'] = 100\n",
        "args['lr'] = 0.01\n",
        "#args['gradient_clip_norm'] = 1.0\n",
        "#args['network_weight_decay'] = 0.0001\n",
        "#args['lr_decay_factor'] = 0.5\n",
        "\n",
        "# check batch size -> to include examples of classes\n",
        "# dropout maybe higher\n"
      ],
      "metadata": {
        "id": "NDM8I94pr9DS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sweeps"
      ],
      "metadata": {
        "id": "NPCwEq9PCwYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "parameters_dict = {\n",
        "    'batch_size': {\n",
        "        'values': [512, 1014]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "parameters_dict.update({\n",
        "    'num_data_points': {\n",
        "        'value': args['num_data_points']},\n",
        "    'num_epochs': {\n",
        "        'value': args['num_epochs']},\n",
        "    'num_layers': {\n",
        "        'value': args['num_layers']},\n",
        "    'hidden_channels': {\n",
        "        'value': args['hidden_channels']},\n",
        "    'lr': {\n",
        "        'value': args['lr']}\n",
        "    })\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "q4KsLRLXN9KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"GDL_molecular_activity_prediction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMXJz4sNuCo",
        "outputId": "97cdebd8-6afc-4b3f-9d2b-62188acf726f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: vcsw0x6q\n",
            "Sweep URL: https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/vcsw0x6q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sweep(data_list, args):\n",
        "    # Create a custom run name dynamically\n",
        "    run_name = f\"{args['model']}_b{args['batch_size']}_d{args['dropout']}\"\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=wandb.config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        args['batch_size'] = config.batch_size\n",
        "        args['dropout'] = config.dropout\n",
        "\n",
        "        # create dataset from data_list\n",
        "        dataloader = prepare_dataloader(data_list, args)\n",
        "\n",
        "        # train model\n",
        "        exp = TrainManager(dataloader, args)\n",
        "        exp.train(epochs=10, log=True, wb_log=True)\n"
      ],
      "metadata": {
        "id": "BinziYUTQwGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args['model'] = 'GROVER_FP'\n",
        "# run the sweep\n",
        "wandb.agent(sweep_id, run_sweep(data_list, args), count=4)"
      ],
      "metadata": {
        "id": "NxiWVecC85c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single run"
      ],
      "metadata": {
        "id": "Mc6kDkF_CyMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args['model'] = 'GROVER_FP'\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['hidden_channels'] = 512\n",
        "args['lr'] = 0.01\n",
        "# API KEY: 69f641df6e6f0934ab302070cf0b3bcd5399ddd3\n",
        "run_name = f\"{args['model']}_b{args['batch_size']}_d{args['dropout']}_hdim{args['hidden_channels']}\"\n",
        "run = wandb.init(\n",
        "    name=run_name,\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"GDL_molecular_activity_prediction\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        'num_data_points': args['num_data_points'],\n",
        "        'assays': 'cell_based_high_hr',\n",
        "        'num_assays': args['num_assays'],\n",
        "\n",
        "        'model': args['model'],\n",
        "        'num_layers': args['num_layers'],\n",
        "        'hidden_channels': args['hidden_channels'],\n",
        "        'dropout': args['dropout'],\n",
        "        'batch_size': args['batch_size'],\n",
        "        'num_epochs': args['num_epochs'],\n",
        "        'lr': args['lr'],\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "LpDabvygfNUy",
        "outputId": "94ca8ef1-5dfc-42e2-8ce5-0cd5b753267f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230815_093938-gmeyfwy1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/gmeyfwy1' target=\"_blank\">GROVER_FP_b256_d0.2_hdim512</a></strong> to <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/gmeyfwy1' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/gmeyfwy1</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset from data_list\n",
        "dataloader = prepare_dataloader(data_list, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxCKJ7OjVhKG",
        "outputId": "cc07b70b-994a-440a-e29b-4f2dd9612ee5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 194514\n",
            "Number of validation graphs: 64838\n",
            "Number of test graphs: 64839\n",
            "Example of a graph data object: Data(x=[29, 79], edge_index=[2, 62], edge_attr=[62, 10], y=[1, 5], fp=[2215], grover_fp=[5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "exp = TrainManager(dataloader, args)\n",
        "exp.train(epochs=100, log=True, wb_log=True)"
      ],
      "metadata": {
        "id": "QiLH8E9KVd1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c914b4-8ca2-4432-cc21-29ad7acf40de"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is on device: cuda:0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [0/100]: 760it [00:20, 37.36it/s]                         \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.2755, Train AUC: 0.4999, Test AUC: 0.4999\n",
            "                        Train F1: 0.0012, Test F1: 0.0012\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/100]: 760it [00:16, 46.52it/s]\n",
            "Epoch [2/100]: 760it [00:16, 46.93it/s]\n",
            "Epoch [3/100]: 760it [00:16, 46.83it/s]\n",
            "Epoch [4/100]: 760it [00:16, 46.70it/s]\n",
            "Epoch [5/100]: 760it [00:16, 46.48it/s]\n",
            "Epoch [6/100]: 760it [00:16, 46.63it/s]\n",
            "Epoch [7/100]: 760it [00:16, 46.54it/s]\n",
            "Epoch [8/100]: 760it [00:16, 47.01it/s]\n",
            "Epoch [9/100]: 760it [00:16, 46.69it/s]\n",
            "Epoch [10/100]: 760it [00:16, 46.13it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 0.2291, Train AUC: 0.5194, Test AUC: 0.5154\n",
            "                        Train F1: 0.0700, Test F1: 0.0582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [11/100]: 760it [00:16, 47.04it/s]\n",
            "Epoch [12/100]: 760it [00:16, 45.67it/s]\n",
            "Epoch [13/100]: 760it [00:16, 45.83it/s]\n",
            "Epoch [14/100]: 760it [00:16, 45.50it/s]                         \n",
            "Epoch [15/100]: 760it [00:16, 45.44it/s]\n",
            "Epoch [16/100]: 760it [00:16, 47.16it/s]\n",
            "Epoch [17/100]: 760it [00:16, 46.89it/s]\n",
            "Epoch [18/100]: 760it [00:16, 46.30it/s]\n",
            "Epoch [19/100]: 760it [00:16, 45.76it/s]\n",
            "Epoch [20/100]: 760it [00:16, 45.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Loss: 0.2474, Train AUC: 0.5395, Test AUC: 0.5291\n",
            "                        Train F1: 0.1237, Test F1: 0.0967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [21/100]: 760it [00:16, 47.06it/s]\n",
            "Epoch [22/100]: 760it [00:16, 45.98it/s]\n",
            "Epoch [23/100]: 760it [00:16, 46.04it/s]\n",
            "Epoch [24/100]: 760it [00:16, 45.40it/s]                         \n",
            "Epoch [25/100]: 760it [00:17, 44.57it/s]\n",
            "Epoch [26/100]: 760it [00:16, 45.26it/s]                         \n",
            "Epoch [27/100]: 760it [00:16, 45.81it/s]\n",
            "Epoch [28/100]: 760it [00:16, 46.29it/s]\n",
            "Epoch [29/100]: 760it [00:16, 46.38it/s]\n",
            "Epoch [30/100]: 760it [00:16, 45.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, Loss: 0.2266, Train AUC: 0.5629, Test AUC: 0.5437\n",
            "                        Train F1: 0.1714, Test F1: 0.1289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [31/100]: 760it [00:16, 46.45it/s]\n",
            "Epoch [32/100]: 760it [00:16, 46.46it/s]\n",
            "Epoch [33/100]: 760it [00:16, 46.43it/s]                         \n",
            "Epoch [34/100]: 760it [00:16, 46.45it/s]\n",
            "Epoch [35/100]: 760it [00:16, 45.80it/s]\n",
            "Epoch [36/100]: 760it [00:16, 46.32it/s]\n",
            "Epoch [37/100]: 760it [00:16, 45.47it/s]\n",
            "Epoch [38/100]: 760it [00:16, 45.04it/s]\n",
            "Epoch [39/100]: 760it [00:16, 46.11it/s]\n",
            "Epoch [40/100]: 760it [00:16, 46.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, Loss: 0.2710, Train AUC: 0.5018, Test AUC: 0.5012\n",
            "                        Train F1: 0.0075, Test F1: 0.0056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [41/100]: 760it [00:16, 45.73it/s]                         \n",
            "Epoch [42/100]: 760it [00:16, 45.05it/s]\n",
            "Epoch [43/100]: 760it [00:16, 46.93it/s]\n",
            "Epoch [44/100]: 760it [00:16, 46.60it/s]\n",
            "Epoch [45/100]: 760it [00:16, 46.51it/s]\n",
            "Epoch [46/100]: 760it [00:16, 46.33it/s]\n",
            "Epoch [47/100]: 760it [00:16, 46.36it/s]\n",
            "Epoch [48/100]: 760it [00:16, 46.54it/s]\n",
            "Epoch [49/100]: 760it [00:16, 46.63it/s]                         \n",
            "Epoch [50/100]: 760it [00:16, 46.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Loss: 0.2550, Train AUC: 0.5733, Test AUC: 0.5520\n",
            "                        Train F1: 0.1778, Test F1: 0.1373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [51/100]: 760it [00:16, 46.66it/s]\n",
            "Epoch [52/100]: 760it [00:16, 46.75it/s]\n",
            "Epoch [53/100]: 760it [00:16, 46.81it/s]\n",
            "Epoch [54/100]: 760it [00:16, 46.99it/s]\n",
            "Epoch [55/100]: 760it [00:16, 46.60it/s]\n",
            "Epoch [56/100]: 760it [00:16, 47.11it/s]\n",
            "Epoch [57/100]: 760it [00:16, 46.72it/s]\n",
            "Epoch [58/100]: 760it [00:16, 46.38it/s]\n",
            "Epoch [59/100]: 760it [00:16, 45.72it/s]\n",
            "Epoch [60/100]: 760it [00:16, 46.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60, Loss: 0.2026, Train AUC: 0.5592, Test AUC: 0.5440\n",
            "                        Train F1: 0.1569, Test F1: 0.1259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [61/100]: 760it [00:16, 46.67it/s]\n",
            "Epoch [62/100]: 760it [00:16, 46.74it/s]\n",
            "Epoch [63/100]: 760it [00:16, 46.94it/s]\n",
            "Epoch [64/100]: 760it [00:16, 46.15it/s]\n",
            "Epoch [65/100]: 760it [00:16, 46.81it/s]\n",
            "Epoch [66/100]: 760it [00:16, 46.74it/s]                         \n",
            "Epoch [67/100]: 760it [00:16, 46.69it/s]\n",
            "Epoch [68/100]: 760it [00:16, 46.62it/s]\n",
            "Epoch [69/100]: 760it [00:16, 45.84it/s]\n",
            "Epoch [70/100]: 760it [00:16, 45.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 70, Loss: 0.2684, Train AUC: 0.5403, Test AUC: 0.5247\n",
            "                        Train F1: 0.1275, Test F1: 0.0850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [71/100]: 760it [00:16, 46.11it/s]                         \n",
            "Epoch [72/100]: 760it [00:16, 45.99it/s]                         \n",
            "Epoch [73/100]: 760it [00:16, 46.32it/s]\n",
            "Epoch [74/100]: 760it [00:16, 46.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp.analyze()"
      ],
      "metadata": {
        "id": "IY-oKyawe1vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "yu38fmhbdGgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}