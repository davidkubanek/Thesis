{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkubanek/Thesis/blob/main/model_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpnAQeFgIIKA"
      },
      "source": [
        "# CONCERTO architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1jm2t5MI4af",
        "outputId": "797f2597-b0bc-45b3-e5b9-c41a939dd8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-1.1.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.2\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.2\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=f96f8180ab40151e91208da8f265006a1493e565deccb1329a3ed857d30310c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=67d82bec38095272f15a38c3f2edc5e43a0d71f3ecd413b5fb3468ab6170e050\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n"
          ]
        }
      ],
      "source": [
        "# for running in colab\n",
        "!pip install dgl\n",
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69lAzjYIIKB",
        "outputId": "9d946359-3615-4514-b5f6-596ec27da9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  plt.rcParams[\"font.family\"] = \"Palatino\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.loader import DataLoader\n",
        "import wandb\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "# check if cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQK8GKzgKVqk",
        "outputId": "56963eed-6b15-4fe9-d1f6-c68d51acd186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG-9VYyW7Dud"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "eR47vh5vf_Se"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def prepare_datalist(matrix_df, args, graph_fp=True, grover_fp=None):\n",
        "    '''\n",
        "    Convert matrix dataframe to a data_list with pytorch geometric graph data, fingerprints and labels.\n",
        "    Inputs:\n",
        "        matrix_df: dataframe of SMILES, assays and bioactivity labels\n",
        "        args: arguments\n",
        "        graph_fp: if True, includes graph embedding fingerprints into data_list\n",
        "        grover_fp: if True, includes GROVER graph transformer embedding fingerprints into data_list\n",
        "    Outputs:\n",
        "        data_list: list of data objects\n",
        "    '''\n",
        "    # only use subset of data (assays and data points)\n",
        "    assay_list = args['assay_list']\n",
        "    num_assays = args['num_assays']\n",
        "    assay_start = args['assay_start']\n",
        "    num_data_points = args['num_data_points']\n",
        "\n",
        "    # get binary target labels\n",
        "    y = matrix_df[assay_list[assay_start:assay_start+num_assays]].values[:num_data_points]\n",
        "\n",
        "    # get SMILES strings\n",
        "    data = matrix_df['SMILES'].values[:num_data_points]\n",
        "\n",
        "    if graph_fp is True: # add graph fingerprint\n",
        "        GraphDataset = GraphDatasetClass()\n",
        "        # create pytorch geometric graph data list\n",
        "        data_list = GraphDataset.create_pytorch_geometric_graph_data_list_from_smiles_and_labels(data, y)\n",
        "    else: # create simple data_list without graph fingerprint\n",
        "      data_list = []\n",
        "      for label in y:\n",
        "          # construct Pytorch Geometric data object and append to data list\n",
        "          data_list.append(Data(y = label.reshape(1, -1)))\n",
        "\n",
        "    # add fingerprint data to each graph\n",
        "    for i, smile in tqdm(enumerate(data), desc='Adding fingerprints...', total=len(data)):\n",
        "        fp = convert_smile_to_fp_bit_string(smile)\n",
        "        data_list[i].fp = fp\n",
        "\n",
        "\n",
        "    # add grover fingerprint to each graph\n",
        "    if grover_fp is not None:\n",
        "        for i, gfp in tqdm(enumerate(grover_fp['fps'][:args['num_data_points']]), desc='Adding grover embedding...', total=len(data)):\n",
        "          data_list[i].grover_fp = torch.tensor(gfp)\n",
        "\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def prepare_splits(data_list, args):\n",
        "\n",
        "    data_list = data_list[:args['num_data_points']]\n",
        "\n",
        "    data_splits = {}\n",
        "    # split into train and test\n",
        "    train_dataset = [d.to(args['device']) for d in data_list[:int(len(data_list)*0.8)]]\n",
        "    data_splits['test'] = [d.to(args['device']) for d in data_list[int(len(data_list)*0.8):]]\n",
        "\n",
        "    # split into train and validation\n",
        "    data_splits['val'] = train_dataset[:int(len(train_dataset)*0.25)]\n",
        "    data_splits['train'] = train_dataset[int(len(train_dataset)*0.25):]\n",
        "\n",
        "    print(f'Number of training graphs:', len(data_splits['train']))\n",
        "    print(f'Number of validation graphs:', len(data_splits['val']))\n",
        "    print(f'Number of test graphs:', len(data_splits['test']))\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "\n",
        "def prepare_dataloader(data_splits, args):\n",
        "    '''\n",
        "    Get dataloader dictionary from data_list with desired batch_size\n",
        "    '''\n",
        "    # create data loaders\n",
        "    dataloader = {}\n",
        "    dataloader['train'] = DataLoader(data_splits['train'], batch_size=args['batch_size'], shuffle=True)\n",
        "    dataloader['val'] = DataLoader(data_splits['val'], batch_size=args['batch_size'], shuffle=False)\n",
        "    dataloader['test'] = DataLoader(data_splits['test'], batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def analyze_dataset(dataset, args):\n",
        "    '''\n",
        "    Analyze the distribution of positive classes in the dataset\n",
        "    '''\n",
        "    positive = []\n",
        "    for i in range(len(dataset)):\n",
        "        positive.append(dataset[i].y[0].sum().item())\n",
        "\n",
        "\n",
        "    num_assays = args['num_assays']\n",
        "    # make histogram of the number of positive\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    # define bins\n",
        "    bins = np.linspace(0, num_assays, num_assays+1)-0.5\n",
        "    plt.hist(positive, bins=bins, alpha=0.5, label='train')\n",
        "    num_assays = args['num_assays']\n",
        "    plt.xlabel(f'# of positive hits in target vector (out of {num_assays})')\n",
        "    plt.ylabel('Number of data points')\n",
        "    plt.title('Histogram of positive class distribution')\n",
        "    plt.show()\n",
        "\n",
        "    # for i in range(num_assays+1):\n",
        "    #     print(f'Number of data points with {i} positive targets: ', (np.array(positive) == i).sum(), f'({(np.array(positive) == i).sum()/len(positive)*100:.2f}%)')\n",
        "\n",
        "def data_explore(dataloader):\n",
        "    '''\n",
        "    Explore the data\n",
        "    '''\n",
        "    # check proportion of positive and negative samples across each assay\n",
        "    pos = torch.zeros(args['num_assays'])\n",
        "    for data in dataloader:  # Iterate in batches over the training dataset\n",
        "        # print('inputs:')\n",
        "        # print(' x:', data.x.shape, '| y:',data.y.shape, '| fp:',data.fp.shape, '| grover:', data.grover_fp.shape)\n",
        "        pos += data.y.sum(axis=0)\n",
        "        #  print(data.y.sum(axis=0))\n",
        "    print('# positive samples:', pos)\n",
        "    print(torch.round((pos/len(dataloader.dataset)*100),decimals=2),'% are positive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bFUFuaaP7MeT"
      },
      "outputs": [],
      "source": [
        " import pickle\n",
        "import os\n",
        "\n",
        "#directory = '/home/ubuntu/Thesis/Thesis MSc/PubChem Data/'\n",
        "directory = '/content/drive/MyDrive/Thesis/Data/'\n",
        "\n",
        "# Specify the path where you saved the dictionary\n",
        "load_path = directory + 'final/datalist_no_out.pkl'\n",
        "\n",
        "# Load the dictionary using pickle\n",
        "with open(load_path, 'rb') as f:\n",
        "    data_list = pickle.load(f)\n",
        "\n",
        "\n",
        "# load the assay groups\n",
        "with open(directory + 'info/cell_based_high_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "cell_based_high_hr = list(map(str, lines))\n",
        "with open(directory + 'info/cell_based_med_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "cell_based_med_hr = list(map(str, lines))\n",
        "with open(directory + 'info/cell_based_low_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "cell_based_low_hr = list(map(str, lines))\n",
        "with open(directory + 'info/non_cell_based_high_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "non_cell_based_high_hr = list(map(str, lines))\n",
        "with open(directory + 'info/non_cell_based_med_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "non_cell_based_med_hr = list(map(str, lines))\n",
        "with open(directory + 'info/non_cell_based_low_hr.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "non_cell_based_low_hr = list(map(str, lines))\n",
        "# load assay order\n",
        "with open(directory + 'info/assay_order.txt', 'r') as f:\n",
        "    assay_order = [line.strip() for line in f.readlines()]\n",
        "\n",
        "args = {}\n",
        "args['assay_order'] = assay_order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtbv6Cb4Mes0"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eEWrR-aTjYI"
      },
      "source": [
        "Define hyperparams to sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWyacYyIIKG"
      },
      "source": [
        "# Models\n",
        "### GCN and GCN_FP\n",
        "- GCN: graph embedding followed by a final classification layer\n",
        "- GCN_FP: graph + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1q43d46hIIKG"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    '''\n",
        "    Define a Graph Convolutional Network (GCN) model architecture.\n",
        "    Can include 'graph' only or 'graph + fingerprints' embedding before final classification layer.\n",
        "    '''\n",
        "    def __init__(self, args):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "\n",
        "        num_node_features = args['num_node_features']\n",
        "        hidden_channels = args['hidden_channels']\n",
        "        num_classes = args['num_assays']\n",
        "        if args['model'] == 'GCN_FP':\n",
        "            fp_dim = args['fp_dim']\n",
        "        else:\n",
        "            fp_dim = 0\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin = Linear(hidden_channels + fp_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, fp=None):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # if also using fingerprints\n",
        "        if fp is not None:\n",
        "            # reshape fp to batch_size x fp_dim\n",
        "            fp = fp.reshape(x.shape[0], -1)\n",
        "            # concatenate graph node embeddings with fingerprint\n",
        "            # print('BEFORE CONCAT x:',x.shape, 'fp:', fp.shape)\n",
        "            x = torch.cat([x, fp], dim=1)\n",
        "            # print('AFTER CONCAT x:',x.shape)\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cENu4SMIIKG"
      },
      "source": [
        "### FP, GROVER and GROVER_FP\n",
        "- FP: fingerprints embedding followed by a final classification layer\n",
        "- GROVER: graph transformer embedding followed by a final classification layer\n",
        "- GROVER_FP: graph transformer + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PXTwqghIIIKG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LinearBlock(nn.Module):\n",
        "\t\"\"\" basic block in an MLP, with dropout and batch norm \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_feats, out_feats, dropout=0.1):\n",
        "\t\tsuper(LinearBlock, self).__init__()\n",
        "\t\tself.linear = nn.Linear(in_feats, out_feats)\n",
        "\t\tself.bn = nn.BatchNorm1d(out_feats)\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t# ReLU activation, batch norm, dropout on layer\n",
        "\t\treturn self.bn(self.dropout(F.relu(self.linear(x))))\n",
        "\n",
        "def construct_mlp(in_dim, out_dim, hidden_dim, hidden_layers, dropout=0.1):\n",
        "\t\"\"\"\n",
        "\tConstructs an MLP with specified dimensions.\n",
        "\t\t- total number of layers = hidden_layers + 1 (the + 1 is for the output linear)\n",
        "\t\t- no activation/batch norm/dropout on output layer\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert hidden_layers >= 1, hidden_layers\n",
        "\tmlp_list = []\n",
        "\tmlp_list.append(LinearBlock(in_dim,hidden_dim,dropout=dropout))\n",
        "\tfor i in range(hidden_layers-1):\n",
        "\t\tmlp_list.append(LinearBlock(hidden_dim,hidden_dim,dropout=dropout))\n",
        "\n",
        "\t# no activation/batch norm/dropout on output layer\n",
        "\tmlp_list.append(nn.Linear(hidden_dim,out_dim))\n",
        "\tmlp = nn.Sequential(*mlp_list)\n",
        "\treturn mlp\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\t'''\n",
        "\tMLP with optional Grover fingerprints.\n",
        "\tCustomizable number of layers, hidden dimensions, and dropout.\n",
        "\t'''\n",
        "\tdef __init__(self, args):\n",
        "\n",
        "\t\tsuper(MLP, self).__init__()\n",
        "\n",
        "\t\tself.model_type = args['model']\n",
        "\t\tself.fp_dim = args['fp_dim'] # can be 0\n",
        "\t\tself.grover_fp_dim = args['grover_fp_dim'] # can be 0\n",
        "\t\tself.hidden_dim = args['hidden_channels']\n",
        "\t\tself.output_dim = args['num_assays']\n",
        "\t\tself.num_layers = args['num_layers']\n",
        "\t\tself.dropout = args['dropout']\n",
        "\n",
        "\t\tassert self.model_type in ['FP','GROVER','GROVER_FP'], f'model type not supported: {self.model_type}'\n",
        "\n",
        "\t\tif self.model_type == 'FP':\n",
        "\t\t\tself.grover_fp_dim = 0\n",
        "\t\telif self.model_type == 'GROVER':\n",
        "\t\t\tself.fp_dim = 0\n",
        "\n",
        "\t\tself.ff_layers = construct_mlp(\n",
        "\t\t\tself.fp_dim + self.grover_fp_dim,\n",
        "\t\t\tself.output_dim,\n",
        "\t\t\tself.hidden_dim,\n",
        "\t\t\tself.num_layers,\n",
        "\t\t\tself.dropout\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\n",
        "\n",
        "\t\tif self.model_type == 'FP': # only fp is used\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(fingerprints)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER': # only grover is used\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(grover_fp)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER_FP': #grover and fp are concatenated\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(torch.cat([fingerprints, grover_fp], dim=1))\n",
        "\n",
        "\n",
        "\t\treturn output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMXuJkTGIIKH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "A3SbXTXlIIKH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "\n",
        "class TrainManager:\n",
        "\n",
        "    def __init__(self, dataloader, args, model=None):\n",
        "\n",
        "        self.args = args\n",
        "        self.num_assays = args['num_assays']\n",
        "        self.num_node_features = args['num_node_features']\n",
        "        self.hidden_channels = args['hidden_channels']\n",
        "\n",
        "        if not model:\n",
        "            # initialize model depending on model type\n",
        "            if args['model'] in ['GCN','GCN_FP']:\n",
        "                self.model = GCN(args)\n",
        "            elif args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "              self.model = MLP(args)\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "        self.model.to(args['device'])\n",
        "        print(\"Model is on device:\", next(self.model.parameters()).device)\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(f'Total number of parameters: {total_params}')\n",
        "\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args['lr'])\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.curr_epoch = 0\n",
        "\n",
        "        # logging\n",
        "        self.eval_metrics = {}\n",
        "        self.eval_metrics['loss'] = []\n",
        "        self.eval_metrics['acc_train'] = []\n",
        "        self.eval_metrics['acc_test'] = []\n",
        "        self.eval_metrics['auc_train'] = []\n",
        "        self.eval_metrics['auc_test'] = []\n",
        "        self.eval_metrics['precision_train'] = []\n",
        "        self.eval_metrics['precision_test'] = []\n",
        "        self.eval_metrics['recall_train'] = []\n",
        "        self.eval_metrics['recall_test'] = []\n",
        "        self.eval_metrics['f1_train'] = []\n",
        "        self.eval_metrics['f1_test'] = []\n",
        "\n",
        "\n",
        "    def train(self, epochs=100, log=False, wb_log=False):\n",
        "        '''\n",
        "        Train the model for a given number of epochs.\n",
        "        '''\n",
        "\n",
        "        self.wb_log = wb_log\n",
        "\n",
        "        epoch_times = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            self.model.train()\n",
        "            cum_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Iterate in batches over the training dataset\n",
        "            for data in tqdm(self.dataloader['train'], desc=f'Epoch [{self.curr_epoch}/{epochs}]', total=int(len(self.dataloader['train'].dataset)/self.args['batch_size'])):\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                # print('data.y:',data.y.shape)\n",
        "                loss = self.criterion(out, data.y[:,args['assays_idx']])  # Compute the loss. (sigmoid inherent in loss)\n",
        "                loss.backward()  # Derive gradients.\n",
        "                self.optimizer.step()  # Update parameters based on gradients.\n",
        "                self.optimizer.zero_grad()  # Clear gradients.\n",
        "                cum_loss += loss.item()\n",
        "\n",
        "            self.eval_metrics['loss'].append(cum_loss/len(self.dataloader['train']))\n",
        "            if wb_log is True:\n",
        "                wandb.log({'epoch': self.curr_epoch, \"loss\": cum_loss/len(self.dataloader['train'])})\n",
        "\n",
        "            epoch_time = time.time() - start_time\n",
        "            epoch_times.append(epoch_time)\n",
        "\n",
        "            if log:\n",
        "                # evaluate\n",
        "                acc_train, auc_train, precision_train, recall_train, f1_train = self.eval(self.dataloader['train'])\n",
        "                acc_test, auc_test, precision_test, recall_test, f1_test = self.eval(self.dataloader['val'])\n",
        "\n",
        "\n",
        "                self.eval_metrics['acc_train'].append(acc_train)\n",
        "                self.eval_metrics['acc_test'].append(acc_test)\n",
        "                self.eval_metrics['auc_train'].append(auc_train)\n",
        "                self.eval_metrics['auc_test'].append(auc_test)\n",
        "                self.eval_metrics['precision_train'].append(precision_train)\n",
        "                self.eval_metrics['precision_test'].append(precision_test)\n",
        "                self.eval_metrics['recall_train'].append(recall_train)\n",
        "                self.eval_metrics['recall_test'].append(recall_test)\n",
        "                self.eval_metrics['f1_train'].append(f1_train)\n",
        "                self.eval_metrics['f1_test'].append(f1_test)\n",
        "\n",
        "                if wb_log is True:\n",
        "                    wandb.log({'epoch': self.curr_epoch, \"AUC train\": auc_train, \"AUC test\": auc_test, \"F1 train\": f1_train, \"F1 test\": f1_test, \"Precision train\": precision_train, \"Precision test\": precision_test, \"Recall train\": recall_train, \"Recall test\": recall_test})\n",
        "\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f'Epoch: {self.curr_epoch}, Loss: {loss.item():.4f}, Train AUC: {auc_train:.4f}, Test AUC: {auc_test:.4f}')\n",
        "                    print(f'                        Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}')\n",
        "\n",
        "            self.curr_epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "        self.avg_epoch_time = np.mean(epoch_times)\n",
        "        if wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"avg epoch time\": self.avg_epoch_time})\n",
        "\n",
        "    def eval(self, loader):\n",
        "        '''\n",
        "        Evaluate the model on a given dataset (train/val/test).\n",
        "        '''\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # print(\"Model is on device for eval:\", next(exp.model.parameters()).device)\n",
        "\n",
        "        correct = 0\n",
        "\n",
        "        gts = []\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "\n",
        "                data = data.to(self.args['device'])\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # convert out to binary\n",
        "                pred = torch.round(torch.sigmoid(out))\n",
        "                preds.append(torch.round(torch.sigmoid(out)).tolist())\n",
        "                gts.append(data.y[:,args['assays_idx']].tolist())\n",
        "                # print('pred:', pred)\n",
        "                # print('data.y:', data.y)\n",
        "                # print('data.y eval:',data.y.shape)\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                correct += int((pred == data.y[:,args['assays_idx']]).sum())  # Check against ground-truth labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        preds = [b[i] for b in preds for i in range(len(b))]\n",
        "        gts = [b[i] for b in gts for i in range(len(b))]\n",
        "\n",
        "        auc = roc_auc_score(gts, preds)\n",
        "        # Calculate macro-averaged precision, recall, and F1 Score\n",
        "        precision = precision_score(gts, preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(gts, preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(gts, preds, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "        acc = correct / (len(loader.dataset) * self.args['num_assays']) # Derive ratio of correct predictions.\n",
        "\n",
        "        self.eval_time = time.time() - start_time\n",
        "\n",
        "        if self.wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"eval time\": self.eval_time})\n",
        "\n",
        "        return acc, auc, precision, recall, f1\n",
        "\n",
        "\n",
        "\n",
        "    def analyze(self):\n",
        "        '''\n",
        "        Plot the model performance.\n",
        "        '''\n",
        "\n",
        "        # plot side by side\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        ax1.plot(self.eval_metrics['loss'])\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Losses')\n",
        "\n",
        "        ax2.plot(self.eval_metrics['auc_train'], label='train')\n",
        "        ax2.plot(self.eval_metrics['auc_test'], label='test')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('AUC')\n",
        "        ax2.set_title('Area Under Curve')\n",
        "        ax2.legend()\n",
        "        # make main title for the whole plot\n",
        "        if args['model'] in ['GCN', 'GCN_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Node feats: {self.args[\"num_node_features\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        elif args['model'] in ['FP', 'GROVER', 'GROVER_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Num layers: {self.args[\"num_layers\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, folder, filename, save_weights=True, save_logs=True):\n",
        "        print('saving experiment...')\n",
        "\n",
        "        filename += f'_{self.curr_epoch}e'\n",
        "        if save_weights:\n",
        "            torch.save(self.model.state_dict(), os.path.join(folder, filename+'.pt'))\n",
        "\n",
        "        #if save_logs:\n",
        "\n",
        "    def load_model(self, folder, filename):\n",
        "        print('loading model...')\n",
        "        self.model.load_state_dict(torch.load(os.path.join(folder, filename+'.pt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxwqEHtDIIKH"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "NDM8I94pr9DS"
      },
      "outputs": [],
      "source": [
        "args = {}\n",
        "args['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# data parameters\n",
        "args['num_data_points'] = 324191 # all=324191 # number of data points to use\n",
        "#args['assay_list'] = cell_based_high_hr #for all: matrix_df.columns.values[1:]\n",
        "args['num_assays'] = 5 # number of assays to use (i.e., no. of output classes)\n",
        "args['assay_start'] = 0 # which assay to start from\n",
        "args['assay_order'] = assay_order\n",
        "args['num_node_features'] = 79 # number of node features in graph representation\n",
        "args['grover_fp_dim'] = 5000 #grover_fp['fps'][0].shape[0] # None  # dim of grover fingerprints\n",
        "args['fp_dim'] = 2215 # dim of fingerprints\n",
        "\n",
        "\n",
        "# training parameters\n",
        "args['model'] = 'GROVER_FP' # 'GCN', 'GCN_FP', 'FP', 'GROVER', 'GROVER_FP'\n",
        "args['num_layers'] = 5 # number of layers in MLP\n",
        "args['hidden_channels'] = 128 # 64\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['num_epochs'] = 100\n",
        "args['lr'] = 0.01\n",
        "#args['gradient_clip_norm'] = 1.0\n",
        "#args['network_weight_decay'] = 0.0001\n",
        "#args['lr_decay_factor'] = 0.5\n",
        "\n",
        "# check batch size -> to include examples of classes\n",
        "# dropout maybe higher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a3veHUzXggq",
        "outputId": "f83a38ea-568c-417f-8bd3-de24ac85633a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2797', '743397', '1979', '602248', '624127', '1910', '2796']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cell_based_high_hr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "GHHZCeMvXeVk"
      },
      "outputs": [],
      "source": [
        "# find indeces of assays in assay_list in assay_order\n",
        "# return list of indeces\n",
        "def find_assay_indeces(assay_list, assay_order):\n",
        "    indeces = []\n",
        "    for assay in assay_list:\n",
        "        indeces.append(assay_order.index(assay))\n",
        "    return indeces\n",
        "\n",
        "args['assay_list'] = ['2797']\n",
        "args['num_assays'] = 1\n",
        "args['assays_idx'] = find_assay_indeces(args['assay_list'], assay_order)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_splits = prepare_splits(data_list, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhn2hx6QqAjC",
        "outputId": "90dec0f4-5c1c-4d6b-83b0-a0be90414ad2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 194514\n",
            "Number of validation graphs: 64838\n",
            "Number of test graphs: 64839\n",
            "Example of a graph data object: Data(x=[24, 79], edge_index=[2, 52], edge_attr=[52, 10], y=[1, 271], fp=[2215], grover_fp=[5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPCwEq9PCwYI"
      },
      "source": [
        "### Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "q4KsLRLXN9KG"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'AUC test'},\n",
        "    }\n",
        "parameters_dict = {\n",
        "    'batch_size': {\n",
        "        'values': [128, 256, 512, 1014]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "parameters_dict.update({\n",
        "    'num_data_points': {\n",
        "        'value': args['num_data_points']},\n",
        "    'num_epochs': {\n",
        "        'value': args['num_epochs']},\n",
        "    'num_layers': {\n",
        "        'value': args['num_layers']},\n",
        "    'hidden_channels': {\n",
        "        'value': args['hidden_channels']},\n",
        "    'lr': {\n",
        "        'value': args['lr']}\n",
        "    })\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwNdYyGgheTU",
        "outputId": "54762017-d3c2-432c-fe51-ba0281f19038"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'method': 'bayes',\n",
              " 'metric': {'goal': 'maximize', 'name': 'AUC test'},\n",
              " 'parameters': {'batch_size': {'values': [128, 256, 512, 1014]},\n",
              "  'dropout': {'values': [0.3, 0.5]},\n",
              "  'num_data_points': {'value': 324191},\n",
              "  'num_epochs': {'value': 100},\n",
              "  'num_layers': {'value': 5},\n",
              "  'hidden_channels': {'value': 128},\n",
              "  'lr': {'value': 0.01}}}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMXJz4sNuCo",
        "outputId": "fb7584fa-9403-42e4-f807-abbc4dbc903b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: kk90ey6b\n",
            "Sweep URL: https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/kk90ey6b\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"GDL_molecular_activity_prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "BinziYUTQwGY"
      },
      "outputs": [],
      "source": [
        "def run_sweep(data_splits, args):\n",
        "    # Create a custom run name dynamically\n",
        "    run_name = f\"{args['model']}\"\n",
        "\n",
        "    with wandb.init(config=args):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "\n",
        "        # with wandb.init(config=wandb.config) as run:\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        # config = wandb.config\n",
        "\n",
        "        args['batch_size'] = wandb.config.batch_size\n",
        "        args['dropout'] = wandb.config.dropout\n",
        "\n",
        "        # create dataset from data_list\n",
        "        dataloader = prepare_dataloader(data_splits, args)\n",
        "\n",
        "        # train model\n",
        "        exp = TrainManager(dataloader, args)\n",
        "        exp.train(epochs=10, log=True, wb_log=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = prepare_dataloader(data_splits, args)"
      ],
      "metadata": {
        "id": "YzkH-H-dos-s"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader['train'].batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONNO-CpSo1eB",
        "outputId": "e5910f5e-61b6-4dd1-f5b7-7ca8b6d405d9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1014"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxiWVecC85c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ed516ad9-6c10-4d64-c440-fadc103c2b2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230816_190103-y1eexngd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/y1eexngd' target=\"_blank\">rural-sweep-4</a></strong> to <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/hqoteomd' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/hqoteomd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/hqoteomd' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/hqoteomd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/y1eexngd' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/y1eexngd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on device: cuda:0\n",
            "Total number of parameters: 991105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [0/10]: 760it [00:19, 39.48it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.1947, Train AUC: 0.5000, Test AUC: 0.5000\n",
            "                        Train F1: 0.4860, Test F1: 0.4852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/10]: 760it [00:18, 41.97it/s]                         \n",
            "Epoch [2/10]: 760it [00:18, 41.97it/s]                         \n",
            "Epoch [3/10]: 760it [00:18, 41.78it/s]                         \n",
            "Epoch [4/10]:   2%|▏         | 13/759 [00:00<00:18, 39.76it/s]"
          ]
        }
      ],
      "source": [
        "args['model'] = 'GROVER_FP'\n",
        "# run the sweep\n",
        "wandb.agent(sweep_id, run_sweep(data_splits, args), count=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc6kDkF_CyMn"
      },
      "source": [
        "### Single run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LpDabvygfNUy",
        "outputId": "14e4922f-7bcf-4045-874e-c99d26d10de1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:a7muhxsq) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC test</td><td>▁▁▁▂▁▁▂▁▁▁▂▁▃▂▂▁▁▁▁▂▂▁▂▁▂▁▁▂▃▁▂█▂▃▁▂▂▅▁▁</td></tr><tr><td>AUC train</td><td>▁▁▁▁▁▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▃▁▂▇▂▃▂▃▂█▁▁</td></tr><tr><td>F1 test</td><td>▁▁▁▁▂▂▂▂▁▂▂▁▃▂▂▁▁▁▁▂▂▁▂▁▂▁▁▂▄▁▃█▂▃▁▂▂▆▁▁</td></tr><tr><td>F1 train</td><td>▁▁▁▁▁▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▂▁▁▁▂▁▂▁▃▁▂▇▂▃▂▃▂█▁▁</td></tr><tr><td>Precision test</td><td>▁▁▂▅▂▂▃▂▂▂▃▁▅▃▃█▁▁▁▂▂▃▃▃▃▃▂▅▃▃▃▅▂▃▂▄▃▃▂▂</td></tr><tr><td>Precision train</td><td>▁▁▃▄▃▃▃▃▂▃▄▃▄▃▃▁▃▄█▃▃▅▃▃▄▄▃▅▅▄▃▇▃▄▃▅▄▇▂▂</td></tr><tr><td>Recall test</td><td>▁▁▁▂▁▁▂▁▁▁▂▁▃▂▂▁▁▁▁▂▂▁▂▁▂▁▁▂▃▁▂█▂▃▁▂▂▅▁▁</td></tr><tr><td>Recall train</td><td>▁▁▁▁▁▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▃▁▂▇▂▃▂▃▂█▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval time</td><td>▆▆▃▆▃▆▃▁█▁▆▁█▁▁▆▁▆▁▆▆▃▇▁▆▃▆█▁▆▁▆▁▁▆▁▆▁▆▁</td></tr><tr><td>loss</td><td>█▆▆▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▄▃▂▂▂▁▂▁▁▁▁▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC test</td><td>0.49973</td></tr><tr><td>AUC train</td><td>0.50079</td></tr><tr><td>F1 test</td><td>0.48799</td></tr><tr><td>F1 train</td><td>0.49006</td></tr><tr><td>Precision test</td><td>0.49362</td></tr><tr><td>Precision train</td><td>0.51639</td></tr><tr><td>Recall test</td><td>0.49973</td></tr><tr><td>Recall train</td><td>0.50079</td></tr><tr><td>epoch</td><td>56</td></tr><tr><td>eval time</td><td>5.67551</td></tr><tr><td>loss</td><td>0.17507</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FP_b256_d0.2_hdim256_ass1979_noout</strong> at: <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/a7muhxsq' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/a7muhxsq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230816_121737-a7muhxsq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:a7muhxsq). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230816_125835-94lbvzgy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/94lbvzgy' target=\"_blank\">GROVER_FP_b256_d0.2_hdim256_ass1979_noout</a></strong> to <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/94lbvzgy' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/94lbvzgy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "args['assay_list'] = ['1979']\n",
        "args['num_assays'] = 1\n",
        "args['assays_idx'] = find_assay_indeces(args['assay_list'], assay_order)\n",
        "\n",
        "args['model'] = 'GROVER_FP'\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['hidden_channels'] = 256\n",
        "args['lr'] = 0.01\n",
        "# Create a custom run name dynamically\n",
        "run_name = f\"{args['model']}_b{args['batch_size']}_d{args['dropout']}_hdim{args['hidden_channels']}_ass{args['assay_list'][0]}_noout\"\n",
        "run = wandb.init(\n",
        "    name=run_name,\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"GDL_molecular_activity_prediction\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        'num_data_points': args['num_data_points'],\n",
        "        'assays': 'cell_based_high_hr',\n",
        "        'num_assays': args['num_assays'],\n",
        "\n",
        "        'model': args['model'],\n",
        "        'num_layers': args['num_layers'],\n",
        "        'hidden_channels': args['hidden_channels'],\n",
        "        'dropout': args['dropout'],\n",
        "        'batch_size': args['batch_size'],\n",
        "        'num_epochs': args['num_epochs'],\n",
        "        'lr': args['lr'],\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxCKJ7OjVhKG",
        "outputId": "aba215d1-1f24-465d-fc60-e55a51f592ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training graphs: 194514\n",
            "Number of validation graphs: 64838\n",
            "Number of test graphs: 64839\n",
            "Example of a graph data object: Data(x=[24, 79], edge_index=[2, 52], edge_attr=[52, 10], y=[1, 271], fp=[2215], grover_fp=[5000])\n"
          ]
        }
      ],
      "source": [
        "# create dataset from data_list\n",
        "dataloader = prepare_dataloader(data_list, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiLH8E9KVd1n",
        "outputId": "89058dbd-21ef-4869-db30-d9e197224ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is on device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [0/100]: 760it [00:17, 43.22it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.1432, Train AUC: 0.5000, Test AUC: 0.5000\n",
            "                        Train F1: 0.4869, Test F1: 0.4870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/100]: 760it [00:17, 42.59it/s]                         \n",
            "Epoch [2/100]: 760it [00:18, 41.87it/s]                         \n",
            "Epoch [3/100]: 760it [00:17, 42.70it/s]                         \n",
            "Epoch [4/100]: 760it [00:17, 42.51it/s]                         \n",
            "Epoch [5/100]: 760it [00:17, 42.41it/s]                         \n",
            "Epoch [6/100]: 760it [00:17, 42.23it/s]                         \n",
            "Epoch [7/100]: 760it [00:18, 41.69it/s]                         \n",
            "Epoch [8/100]: 760it [00:18, 41.62it/s]                         \n",
            "Epoch [9/100]: 760it [00:18, 41.34it/s]                         \n",
            "Epoch [10/100]: 760it [00:18, 41.25it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 0.1985, Train AUC: 0.5008, Test AUC: 0.4998\n",
            "                        Train F1: 0.4902, Test F1: 0.4887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [11/100]: 760it [00:17, 43.05it/s]                         \n",
            "Epoch [12/100]: 760it [00:17, 43.46it/s]                         \n",
            "Epoch [13/100]: 760it [00:17, 42.82it/s]                         \n",
            "Epoch [14/100]: 760it [00:17, 42.87it/s]                         \n",
            "Epoch [15/100]: 760it [00:17, 42.67it/s]                         \n",
            "Epoch [16/100]: 760it [00:17, 42.31it/s]                         \n",
            "Epoch [17/100]: 760it [00:17, 42.64it/s]                         \n",
            "Epoch [18/100]: 760it [00:17, 42.75it/s]                         \n",
            "Epoch [19/100]: 760it [00:17, 42.88it/s]                         \n",
            "Epoch [20/100]: 760it [00:17, 42.81it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20, Loss: 0.2938, Train AUC: 0.5005, Test AUC: 0.4998\n",
            "                        Train F1: 0.4900, Test F1: 0.4886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [21/100]: 760it [00:18, 42.05it/s]                         \n",
            "Epoch [22/100]: 760it [00:17, 42.36it/s]                         \n",
            "Epoch [23/100]: 760it [00:18, 41.18it/s]                         \n",
            "Epoch [24/100]: 760it [00:18, 40.33it/s]                         \n",
            "Epoch [25/100]: 760it [00:18, 40.32it/s]\n",
            "Epoch [26/100]: 760it [00:18, 40.52it/s]                         \n",
            "Epoch [27/100]: 760it [00:18, 40.26it/s]                         \n",
            "Epoch [28/100]: 760it [00:18, 40.45it/s]                         \n",
            "Epoch [29/100]: 760it [00:18, 40.52it/s]                         \n",
            "Epoch [30/100]: 760it [00:18, 40.37it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30, Loss: 0.1760, Train AUC: 0.5002, Test AUC: 0.5000\n",
            "                        Train F1: 0.4885, Test F1: 0.4884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [31/100]: 760it [00:18, 40.28it/s]                         \n",
            "Epoch [32/100]: 760it [00:18, 40.61it/s]                         \n",
            "Epoch [33/100]: 760it [00:18, 40.76it/s]                         \n",
            "Epoch [34/100]: 760it [00:18, 40.37it/s]\n",
            "Epoch [35/100]: 760it [00:18, 40.52it/s]                         \n",
            "Epoch [36/100]: 760it [00:18, 40.64it/s]                         \n",
            "Epoch [37/100]: 760it [00:18, 40.26it/s]                         \n",
            "Epoch [38/100]: 760it [00:19, 38.82it/s]                         \n",
            "Epoch [39/100]: 760it [00:18, 40.16it/s]                         \n",
            "Epoch [40/100]: 760it [00:18, 40.27it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40, Loss: 0.2425, Train AUC: 0.4999, Test AUC: 0.4999\n",
            "                        Train F1: 0.4869, Test F1: 0.4870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [41/100]: 760it [00:18, 40.00it/s]                         \n",
            "Epoch [42/100]: 760it [00:19, 39.77it/s]                         \n",
            "Epoch [43/100]: 760it [00:18, 40.48it/s]\n",
            "Epoch [44/100]: 760it [00:18, 40.37it/s]                         \n",
            "Epoch [45/100]: 760it [00:18, 41.47it/s]\n",
            "Epoch [46/100]: 760it [00:18, 41.60it/s]                         \n",
            "Epoch [47/100]: 760it [00:18, 40.58it/s]\n",
            "Epoch [48/100]: 760it [00:19, 39.78it/s]                         \n",
            "Epoch [49/100]: 760it [00:19, 39.82it/s]                         \n",
            "Epoch [50/100]: 760it [00:19, 39.99it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50, Loss: 0.1454, Train AUC: 0.5000, Test AUC: 0.4994\n",
            "                        Train F1: 0.4880, Test F1: 0.4872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [51/100]: 760it [00:19, 39.75it/s]                         \n",
            "Epoch [52/100]: 760it [00:19, 39.61it/s]\n",
            "Epoch [53/100]: 760it [00:19, 39.63it/s]                         \n",
            "Epoch [54/100]: 760it [00:19, 39.57it/s]                         \n",
            "Epoch [55/100]: 760it [00:19, 39.67it/s]                         \n",
            "Epoch [56/100]: 760it [00:19, 40.00it/s]                         \n",
            "Epoch [57/100]: 760it [00:18, 40.57it/s]\n",
            "Epoch [58/100]: 760it [00:18, 41.03it/s]                         \n",
            "Epoch [59/100]: 760it [00:17, 42.51it/s]                         \n",
            "Epoch [60/100]: 760it [00:18, 41.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 60, Loss: 0.1880, Train AUC: 0.5009, Test AUC: 0.5009\n",
            "                        Train F1: 0.4909, Test F1: 0.4909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [61/100]: 760it [00:20, 37.85it/s]\n",
            "Epoch [62/100]: 760it [00:19, 39.37it/s]                         \n",
            "Epoch [63/100]: 760it [00:19, 39.21it/s]                         \n",
            "Epoch [64/100]: 760it [00:19, 39.67it/s]                         \n",
            "Epoch [65/100]: 760it [00:19, 39.60it/s]                         \n",
            "Epoch [66/100]: 760it [00:18, 40.07it/s]                         \n",
            "Epoch [67/100]: 760it [00:18, 40.43it/s]                         \n",
            "Epoch [68/100]: 760it [00:19, 39.48it/s]                         \n",
            "Epoch [69/100]: 760it [00:18, 40.08it/s]                         \n",
            "Epoch [70/100]: 760it [00:19, 39.99it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 70, Loss: 0.1858, Train AUC: 0.4999, Test AUC: 0.5000\n",
            "                        Train F1: 0.4870, Test F1: 0.4873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [71/100]: 760it [00:18, 40.14it/s]\n",
            "Epoch [72/100]: 760it [00:18, 41.59it/s]                         \n",
            "Epoch [73/100]: 760it [00:18, 41.66it/s]                         \n",
            "Epoch [74/100]: 760it [00:19, 39.49it/s]                         \n",
            "Epoch [75/100]: 760it [00:18, 41.65it/s]\n",
            "Epoch [76/100]: 760it [00:18, 41.45it/s]                         \n",
            "Epoch [77/100]: 760it [00:18, 40.61it/s]                         \n",
            "Epoch [78/100]: 760it [00:18, 41.29it/s]                         \n",
            "Epoch [79/100]: 760it [00:18, 41.24it/s]                         \n",
            "Epoch [80/100]: 760it [00:18, 41.56it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 80, Loss: 0.1888, Train AUC: 0.4994, Test AUC: 0.4997\n",
            "                        Train F1: 0.4873, Test F1: 0.4882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [81/100]: 760it [00:18, 41.66it/s]                         \n",
            "Epoch [82/100]: 760it [00:18, 41.39it/s]                         \n",
            "Epoch [83/100]: 760it [00:18, 41.47it/s]                         \n",
            "Epoch [84/100]: 760it [00:18, 41.10it/s]                         \n",
            "Epoch [85/100]: 760it [00:18, 41.65it/s]                         \n",
            "Epoch [86/100]: 760it [00:18, 41.27it/s]                         \n",
            "Epoch [87/100]: 760it [00:18, 41.87it/s]                         \n",
            "Epoch [88/100]: 760it [00:18, 41.70it/s]                         \n",
            "Epoch [89/100]: 760it [00:18, 41.72it/s]                         \n",
            "Epoch [90/100]: 760it [00:18, 40.70it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 90, Loss: 0.1568, Train AUC: 0.5001, Test AUC: 0.5001\n",
            "                        Train F1: 0.4873, Test F1: 0.4873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [91/100]: 760it [00:18, 41.46it/s]\n",
            "Epoch [92/100]: 760it [00:18, 41.86it/s]                         \n",
            "Epoch [93/100]: 760it [00:18, 41.70it/s]                         \n",
            "Epoch [94/100]: 760it [00:18, 41.69it/s]                         \n",
            "Epoch [95/100]: 760it [00:19, 39.57it/s]\n",
            "Epoch [96/100]: 760it [00:18, 40.49it/s]\n",
            "Epoch [97/100]: 760it [00:17, 42.81it/s]                         \n",
            "Epoch [98/100]: 760it [00:17, 42.66it/s]                         \n",
            "Epoch [99/100]: 760it [00:17, 42.46it/s]                         \n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "exp = TrainManager(dataloader, args)\n",
        "exp.train(epochs=100, log=True, wb_log=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY-oKyawe1vq"
      },
      "outputs": [],
      "source": [
        "exp.analyze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu38fmhbdGgS"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}