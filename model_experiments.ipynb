{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkubanek/Thesis/blob/main/model_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpnAQeFgIIKA"
      },
      "source": [
        "# CONCERTO architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1jm2t5MI4af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440b7a61-c5c5-4b85-c157-efa8a31692eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.2\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=b59ec30fae53abe12164cae2c2b83b030be4850c1db74a5fff11e9ccacae72fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=6ee87ab900beb70ec5b636740ff4c051d1ca2c18184c23f3152a3145d694201e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n"
          ]
        }
      ],
      "source": [
        "# for running in colab\n",
        "!pip install dgl\n",
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69lAzjYIIKB",
        "outputId": "404a1c62-0e1a-4371-9d10-7f7e6542dc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  plt.rcParams[\"font.family\"] = \"Palatino\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.loader import DataLoader\n",
        "import wandb\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "# check if cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQK8GKzgKVqk",
        "outputId": "8386ef49-c1f8-4b75-c866-11f590e6c3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "MG-9VYyW7Dud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def prepare_datalist(matrix_df, args, graph_fp=True, grover_fp=None):\n",
        "    '''\n",
        "    Convert matrix dataframe to a data_list with pytorch geometric graph data, fingerprints and labels.\n",
        "    Inputs:\n",
        "        matrix_df: dataframe of SMILES, assays and bioactivity labels\n",
        "        args: arguments\n",
        "        graph_fp: if True, includes graph embedding fingerprints into data_list\n",
        "        grover_fp: if True, includes GROVER graph transformer embedding fingerprints into data_list\n",
        "    Outputs:\n",
        "        data_list: list of data objects\n",
        "    '''\n",
        "    # only use subset of data (assays and data points)\n",
        "    assay_list = args['assay_list']\n",
        "    num_assays = args['num_assays']\n",
        "    assay_start = args['assay_start']\n",
        "    num_data_points = args['num_data_points']\n",
        "\n",
        "    # get binary target labels\n",
        "    y = matrix_df[assay_list[assay_start:assay_start+num_assays]].values[:num_data_points]\n",
        "\n",
        "    # get SMILES strings\n",
        "    data = matrix_df['SMILES'].values[:num_data_points]\n",
        "\n",
        "    if graph_fp is True: # add graph fingerprint\n",
        "        GraphDataset = GraphDatasetClass()\n",
        "        # create pytorch geometric graph data list\n",
        "        data_list = GraphDataset.create_pytorch_geometric_graph_data_list_from_smiles_and_labels(data, y)\n",
        "    else: # create simple data_list without graph fingerprint\n",
        "      data_list = []\n",
        "      for label in y:\n",
        "          # construct Pytorch Geometric data object and append to data list\n",
        "          data_list.append(Data(y = label.reshape(1, -1)))\n",
        "\n",
        "    # add fingerprint data to each graph\n",
        "    for i, smile in tqdm(enumerate(data), desc='Adding fingerprints...', total=len(data)):\n",
        "        fp = convert_smile_to_fp_bit_string(smile)\n",
        "        data_list[i].fp = fp\n",
        "\n",
        "\n",
        "    # add grover fingerprint to each graph\n",
        "    if grover_fp is not None:\n",
        "        for i, gfp in tqdm(enumerate(grover_fp['fps'][:args['num_data_points']]), desc='Adding grover embedding...', total=len(data)):\n",
        "          data_list[i].grover_fp = torch.tensor(gfp)\n",
        "\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    return data_list\n",
        "\n",
        "def prepare_dataloader(data_list, args):\n",
        "    '''\n",
        "    Get dataloader dictionary from data_list with desired batch_size\n",
        "    '''\n",
        "    data_list = data_list[:args['num_data_points']]\n",
        "    # split into train and test\n",
        "    train_dataset = data_list[:int(len(data_list)*0.8)]\n",
        "    test_dataset = data_list[int(len(data_list)*0.8):]\n",
        "\n",
        "    # split into train and validation\n",
        "    val_dataset = train_dataset[:int(len(train_dataset)*0.25)]\n",
        "    train_dataset = train_dataset[int(len(train_dataset)*0.25):]\n",
        "\n",
        "    print(f'Number of training graphs: {len(train_dataset)}')\n",
        "    print(f'Number of validation graphs: {len(val_dataset)}')\n",
        "    print(f'Number of test graphs: {len(test_dataset)}')\n",
        "    print(f'Example of a graph data object: {data_list[0]}')\n",
        "\n",
        "    # create data loaders\n",
        "    dataloader = {}\n",
        "    dataloader['train'] = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
        "    dataloader['val'] = DataLoader(val_dataset, batch_size=args['batch_size'], shuffle=False)\n",
        "    dataloader['test'] = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def analyze_dataset(dataset, args):\n",
        "    '''\n",
        "    Analyze the distribution of positive classes in the dataset\n",
        "    '''\n",
        "    positive = []\n",
        "    for i in range(len(dataset)):\n",
        "        positive.append(dataset[i].y[0].sum().item())\n",
        "\n",
        "\n",
        "    num_assays = args['num_assays']\n",
        "    # make histogram of the number of positive\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    # define bins\n",
        "    bins = np.linspace(0, num_assays, num_assays+1)-0.5\n",
        "    plt.hist(positive, bins=bins, alpha=0.5, label='train')\n",
        "    num_assays = args['num_assays']\n",
        "    plt.xlabel(f'# of positive hits in target vector (out of {num_assays})')\n",
        "    plt.ylabel('Number of data points')\n",
        "    plt.title('Histogram of positive class distribution')\n",
        "    plt.show()\n",
        "\n",
        "    for i in range(num_assays+1):\n",
        "        print(f'Number of data points with {i} positive targets: ', (np.array(positive) == i).sum(), f'({(np.array(positive) == i).sum()/len(positive)*100:.2f}%)')\n",
        "\n",
        "def data_explore(dataloader):\n",
        "    '''\n",
        "    Explore the data\n",
        "    '''\n",
        "    # check proportion of positive and negative samples across each assay\n",
        "    pos = torch.zeros(args['num_assays'])\n",
        "    for data in dataloader:  # Iterate in batches over the training dataset\n",
        "        # print('inputs:')\n",
        "        # print(' x:', data.x.shape, '| y:',data.y.shape, '| fp:',data.fp.shape, '| grover:', data.grover_fp.shape)\n",
        "        pos += data.y.sum(axis=0)\n",
        "        #  print(data.y.sum(axis=0))\n",
        "    print('# positive samples:', pos)\n",
        "    print(torch.round((pos/len(dataloader.dataset)*100),decimals=2),'% are positive')\n",
        "\n"
      ],
      "metadata": {
        "id": "eR47vh5vf_Se"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "directory = '/content/drive/MyDrive/Thesis/Data/'\n",
        "\n",
        "# Specify the path where you saved the dictionary\n",
        "load_path = directory + 'datalist.pkl'\n",
        "\n",
        "# Load the dictionary using pickle\n",
        "with open(load_path, 'rb') as f:\n",
        "    data_list = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "bFUFuaaP7MeT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "dtbv6Cb4Mes0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define hyperparams to sweep"
      ],
      "metadata": {
        "id": "5eEWrR-aTjYI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWyacYyIIKG"
      },
      "source": [
        "# Models\n",
        "### GCN and GCN_FP\n",
        "- GCN: graph embedding followed by a final classification layer\n",
        "- GCN_FP: graph + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1q43d46hIIKG"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    '''\n",
        "    Define a Graph Convolutional Network (GCN) model architecture.\n",
        "    Can include 'graph' only or 'graph + fingerprints' embedding before final classification layer.\n",
        "    '''\n",
        "    def __init__(self, args):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "\n",
        "        num_node_features = args['num_node_features']\n",
        "        hidden_channels = args['hidden_channels']\n",
        "        num_classes = args['num_assays']\n",
        "        if args['model'] == 'GCN_FP':\n",
        "            fp_dim = args['fp_dim']\n",
        "        else:\n",
        "            fp_dim = 0\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin = Linear(hidden_channels + fp_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, fp=None):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # if also using fingerprints\n",
        "        if fp is not None:\n",
        "            # reshape fp to batch_size x fp_dim\n",
        "            fp = fp.reshape(x.shape[0], -1)\n",
        "            # concatenate graph node embeddings with fingerprint\n",
        "            # print('BEFORE CONCAT x:',x.shape, 'fp:', fp.shape)\n",
        "            x = torch.cat([x, fp], dim=1)\n",
        "            # print('AFTER CONCAT x:',x.shape)\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cENu4SMIIKG"
      },
      "source": [
        "### FP, GROVER and GROVER_FP\n",
        "- FP: fingerprints embedding followed by a final classification layer\n",
        "- GROVER: graph transformer embedding followed by a final classification layer\n",
        "- GROVER_FP: graph transformer + fingerprints embedding followed by a final classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PXTwqghIIIKG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LinearBlock(nn.Module):\n",
        "\t\"\"\" basic block in an MLP, with dropout and batch norm \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_feats, out_feats, dropout=0.1):\n",
        "\t\tsuper(LinearBlock, self).__init__()\n",
        "\t\tself.linear = nn.Linear(in_feats, out_feats)\n",
        "\t\tself.bn = nn.BatchNorm1d(out_feats)\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t# ReLU activation, batch norm, dropout on layer\n",
        "\t\treturn self.bn(self.dropout(F.relu(self.linear(x))))\n",
        "\n",
        "def construct_mlp(in_dim, out_dim, hidden_dim, hidden_layers, dropout=0.1):\n",
        "\t\"\"\"\n",
        "\tConstructs an MLP with specified dimensions.\n",
        "\t\t- total number of layers = hidden_layers + 1 (the + 1 is for the output linear)\n",
        "\t\t- no activation/batch norm/dropout on output layer\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert hidden_layers >= 1, hidden_layers\n",
        "\tmlp_list = []\n",
        "\tmlp_list.append(LinearBlock(in_dim,hidden_dim,dropout=dropout))\n",
        "\tfor i in range(hidden_layers-1):\n",
        "\t\tmlp_list.append(LinearBlock(hidden_dim,hidden_dim,dropout=dropout))\n",
        "\n",
        "\t# no activation/batch norm/dropout on output layer\n",
        "\tmlp_list.append(nn.Linear(hidden_dim,out_dim))\n",
        "\tmlp = nn.Sequential(*mlp_list)\n",
        "\treturn mlp\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\t'''\n",
        "\tMLP with optional Grover fingerprints.\n",
        "\tCustomizable number of layers, hidden dimensions, and dropout.\n",
        "\t'''\n",
        "\tdef __init__(self, args):\n",
        "\n",
        "\t\tsuper(MLP, self).__init__()\n",
        "\n",
        "\t\tself.model_type = args['model']\n",
        "\t\tself.fp_dim = args['fp_dim'] # can be 0\n",
        "\t\tself.grover_fp_dim = args['grover_fp_dim'] # can be 0\n",
        "\t\tself.hidden_dim = args['hidden_channels']\n",
        "\t\tself.output_dim = args['num_assays']\n",
        "\t\tself.num_layers = args['num_layers']\n",
        "\t\tself.dropout = args['dropout']\n",
        "\n",
        "\t\tassert self.model_type in ['FP','GROVER','GROVER_FP'], f'model type not supported: {self.model_type}'\n",
        "\n",
        "\t\tif self.model_type == 'FP':\n",
        "\t\t\tself.grover_fp_dim = 0\n",
        "\t\telif self.model_type == 'GROVER':\n",
        "\t\t\tself.fp_dim = 0\n",
        "\n",
        "\t\tself.ff_layers = construct_mlp(\n",
        "\t\t\tself.fp_dim + self.grover_fp_dim,\n",
        "\t\t\tself.output_dim,\n",
        "\t\t\tself.hidden_dim,\n",
        "\t\t\tself.num_layers,\n",
        "\t\t\tself.dropout\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\n",
        "\n",
        "\t\tif self.model_type == 'FP': # only fp is used\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(fingerprints)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER': # only grover is used\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(grover_fp)\n",
        "\n",
        "\t\telif self.model_type == 'GROVER_FP': #grover and fp are concatenated\n",
        "\t\t\tfingerprints = data.fp\n",
        "\t\t\t# reshape fp to batch_size x fp_dim\n",
        "\t\t\tfingerprints = fingerprints.reshape(int(fingerprints.shape[0]/self.fp_dim), -1)\n",
        "\t\t\t# reshape grover_fp to batch_size x grover_fp_dim\n",
        "\t\t\tgrover_fp = data.grover_fp\n",
        "\t\t\tgrover_fp = grover_fp.reshape(int(grover_fp.shape[0]/self.grover_fp_dim), -1)\n",
        "\n",
        "\t\t\toutput = self.ff_layers(torch.cat([fingerprints, grover_fp], dim=1))\n",
        "\n",
        "\n",
        "\t\treturn output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMXuJkTGIIKH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A3SbXTXlIIKH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "\n",
        "class TrainManager:\n",
        "\n",
        "    def __init__(self, dataloader, args, model=None):\n",
        "\n",
        "        self.args = args\n",
        "        self.num_assays = args['num_assays']\n",
        "        self.num_node_features = args['num_node_features']\n",
        "        self.hidden_channels = args['hidden_channels']\n",
        "\n",
        "        if not model:\n",
        "            # initialize model depending on model type\n",
        "            if args['model'] in ['GCN','GCN_FP']:\n",
        "                self.model = GCN(args)\n",
        "            elif args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "              self.model = MLP(args)\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "        self.model.to(args['device'])\n",
        "        print(\"Model is on device:\", next(self.model.parameters()).device)\n",
        "\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args['lr'])\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.curr_epoch = 0\n",
        "\n",
        "        # logging\n",
        "        self.eval_metrics = {}\n",
        "        self.eval_metrics['loss'] = []\n",
        "        self.eval_metrics['acc_train'] = []\n",
        "        self.eval_metrics['acc_test'] = []\n",
        "        self.eval_metrics['auc_train'] = []\n",
        "        self.eval_metrics['auc_test'] = []\n",
        "        self.eval_metrics['precision_train'] = []\n",
        "        self.eval_metrics['precision_test'] = []\n",
        "        self.eval_metrics['recall_train'] = []\n",
        "        self.eval_metrics['recall_test'] = []\n",
        "        self.eval_metrics['f1_train'] = []\n",
        "        self.eval_metrics['f1_test'] = []\n",
        "\n",
        "\n",
        "    def train(self, epochs=100, log=False, wb_log=False):\n",
        "        '''\n",
        "        Train the model for a given number of epochs.\n",
        "        '''\n",
        "\n",
        "        self.wb_log = wb_log\n",
        "\n",
        "        epoch_times = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            self.model.train()\n",
        "            cum_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Iterate in batches over the training dataset\n",
        "            for data in tqdm(self.dataloader['train'], desc=f'Epoch [{self.curr_epoch}/{epochs}]', total=int(len(self.dataloader['train'].dataset)/self.args['batch_size'])):\n",
        "\n",
        "                data = data.to(self.args['device'])\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                # print('data.y:',data.y.shape)\n",
        "                loss = self.criterion(out, data.y)  # Compute the loss. (sigmoid inherent in loss)\n",
        "                loss.backward()  # Derive gradients.\n",
        "                self.optimizer.step()  # Update parameters based on gradients.\n",
        "                self.optimizer.zero_grad()  # Clear gradients.\n",
        "                cum_loss += loss.item()\n",
        "\n",
        "            self.eval_metrics['loss'].append(cum_loss/len(self.dataloader['train']))\n",
        "            if wb_log is True:\n",
        "                wandb.log({'epoch': self.curr_epoch, \"loss\": cum_loss/len(self.dataloader['train'])})\n",
        "\n",
        "            epoch_time = time.time() - start_time\n",
        "            epoch_times.append(epoch_time)\n",
        "\n",
        "            if log:\n",
        "                # evaluate\n",
        "                acc_train, auc_train, precision_train, recall_train, f1_train = self.eval(self.dataloader['train'])\n",
        "                acc_test, auc_test, precision_test, recall_test, f1_test = self.eval(self.dataloader['val'])\n",
        "\n",
        "\n",
        "                self.eval_metrics['acc_train'].append(acc_train)\n",
        "                self.eval_metrics['acc_test'].append(acc_test)\n",
        "                self.eval_metrics['auc_train'].append(auc_train)\n",
        "                self.eval_metrics['auc_test'].append(auc_test)\n",
        "                self.eval_metrics['precision_train'].append(precision_train)\n",
        "                self.eval_metrics['precision_test'].append(precision_test)\n",
        "                self.eval_metrics['recall_train'].append(recall_train)\n",
        "                self.eval_metrics['recall_test'].append(recall_test)\n",
        "                self.eval_metrics['f1_train'].append(f1_train)\n",
        "                self.eval_metrics['f1_test'].append(f1_test)\n",
        "\n",
        "                if wb_log is True:\n",
        "                    wandb.log({'epoch': self.curr_epoch, \"AUC train\": auc_train, \"AUC test\": auc_test, \"F1 train\": f1_train, \"F1 test\": f1_test, \"Precision train\": precision_train, \"Precision test\": precision_test, \"Recall train\": recall_train, \"Recall test\": recall_test})\n",
        "\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f'Epoch: {self.curr_epoch}, Loss: {loss.item():.4f}, Train AUC: {auc_train:.4f}, Test AUC: {auc_test:.4f}')\n",
        "                    print(f'                        Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}')\n",
        "\n",
        "            self.curr_epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "        self.avg_epoch_time = np.mean(epoch_times)\n",
        "        if wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"avg epoch time\": self.avg_epoch_time})\n",
        "\n",
        "    def eval(self, loader):\n",
        "        '''\n",
        "        Evaluate the model on a given dataset (train/val/test).\n",
        "        '''\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # print(\"Model is on device for eval:\", next(exp.model.parameters()).device)\n",
        "\n",
        "        correct = 0\n",
        "\n",
        "        gts = []\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "\n",
        "                data = data.to(self.args['device'])\n",
        "\n",
        "                # forward pass based on model type\n",
        "                if self.args['model'] == 'GCN':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch)\n",
        "                elif self.args['model'] == 'GCN_FP':\n",
        "                    out = self.model(data.x, data.edge_index, data.batch, fp=data.fp)\n",
        "                elif self.args['model'] in ['FP','GROVER','GROVER_FP']:\n",
        "                    out = self.model(data)\n",
        "\n",
        "                # convert out to binary\n",
        "                pred = torch.round(torch.sigmoid(out))\n",
        "                preds.append(torch.round(torch.sigmoid(out)).tolist())\n",
        "                gts.append(data.y.tolist())\n",
        "                # print('pred:', pred)\n",
        "                # print('data.y:', data.y)\n",
        "                # print('data.y eval:',data.y.shape)\n",
        "                # data.y = data.y.unsqueeze(1)\n",
        "                correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        preds = [b[i] for b in preds for i in range(len(b))]\n",
        "        gts = [b[i] for b in gts for i in range(len(b))]\n",
        "\n",
        "        auc = roc_auc_score(gts, preds)\n",
        "        # Calculate macro-averaged precision, recall, and F1 Score\n",
        "        precision = precision_score(gts, preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(gts, preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(gts, preds, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "        acc = correct / (len(loader.dataset) * self.args['num_assays']) # Derive ratio of correct predictions.\n",
        "\n",
        "        self.eval_time = time.time() - start_time\n",
        "\n",
        "        if self.wb_log is True:\n",
        "            wandb.log({'epoch': self.curr_epoch, \"eval time\": self.eval_time})\n",
        "\n",
        "        return acc, auc, precision, recall, f1\n",
        "\n",
        "\n",
        "\n",
        "    def analyze(self):\n",
        "        '''\n",
        "        Plot the model performance.\n",
        "        '''\n",
        "\n",
        "        # plot side by side\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        ax1.plot(self.eval_metrics['loss'])\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Losses')\n",
        "\n",
        "        ax2.plot(self.eval_metrics['auc_train'], label='train')\n",
        "        ax2.plot(self.eval_metrics['auc_test'], label='test')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('AUC')\n",
        "        ax2.set_title('Area Under Curve')\n",
        "        ax2.legend()\n",
        "        # make main title for the whole plot\n",
        "        if args['model'] in ['GCN', 'GCN_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Node feats: {self.args[\"num_node_features\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        elif args['model'] in ['FP', 'GROVER', 'GROVER_FP']:\n",
        "            plt.suptitle(f'Model: {self.args[\"model\"]} | Num layers: {self.args[\"num_layers\"]}, Hidden dim: {self.args[\"hidden_channels\"]}, Dropout: {self.args[\"dropout\"]}, Num data points: {self.args[\"num_data_points\"]}, Num assays: {self.args[\"num_assays\"]}, Num epochs: {self.curr_epoch}')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, folder, filename, save_weights=True, save_logs=True):\n",
        "        print('saving experiment...')\n",
        "\n",
        "        filename += f'_{self.curr_epoch}e'\n",
        "        if save_weights:\n",
        "            torch.save(self.model.state_dict(), os.path.join(folder, filename+'.pt'))\n",
        "\n",
        "        #if save_logs:\n",
        "\n",
        "    def load_model(self, folder, filename):\n",
        "        print('loading model...')\n",
        "        self.model.load_state_dict(torch.load(os.path.join(folder, filename+'.pt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxwqEHtDIIKH"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {}\n",
        "args['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# data parameters\n",
        "args['num_data_points'] = 324191 # all=324191 # number of data points to use\n",
        "#args['assay_list'] = cell_based_high_hr #for all: matrix_df.columns.values[1:]\n",
        "args['num_assays'] = 5 # number of assays to use (i.e., no. of output classes)\n",
        "args['assay_start'] = 0 # which assay to start from\n",
        "\n",
        "args['num_node_features'] = 79 # number of node features in graph representation\n",
        "args['grover_fp_dim'] = 5000 #grover_fp['fps'][0].shape[0] # None  # dim of grover fingerprints\n",
        "args['fp_dim'] = 2215 # dim of fingerprints\n",
        "\n",
        "\n",
        "# training parameters\n",
        "args['model'] = 'GROVER_FP' # 'GCN', 'GCN_FP', 'FP', 'GROVER', 'GROVER_FP'\n",
        "args['num_layers'] = 5 # number of layers in MLP\n",
        "args['hidden_channels'] = 128 # 64\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['num_epochs'] = 100\n",
        "args['lr'] = 0.01\n",
        "#args['gradient_clip_norm'] = 1.0\n",
        "#args['network_weight_decay'] = 0.0001\n",
        "#args['lr_decay_factor'] = 0.5\n",
        "\n",
        "# check batch size -> to include examples of classes\n",
        "# dropout maybe higher\n"
      ],
      "metadata": {
        "id": "NDM8I94pr9DS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sweeps"
      ],
      "metadata": {
        "id": "NPCwEq9PCwYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "parameters_dict = {\n",
        "    'batch_size': {\n",
        "        'values': [512, 1014]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "parameters_dict.update({\n",
        "    'num_data_points': {\n",
        "        'value': args['num_data_points']},\n",
        "    'num_epochs': {\n",
        "        'value': args['num_epochs']},\n",
        "    'num_layers': {\n",
        "        'value': args['num_layers']},\n",
        "    'hidden_channels': {\n",
        "        'value': args['hidden_channels']},\n",
        "    'lr': {\n",
        "        'value': args['lr']}\n",
        "    })\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "q4KsLRLXN9KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"GDL_molecular_activity_prediction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMXJz4sNuCo",
        "outputId": "97cdebd8-6afc-4b3f-9d2b-62188acf726f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: vcsw0x6q\n",
            "Sweep URL: https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/sweeps/vcsw0x6q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sweep(data_list, args):\n",
        "    # Create a custom run name dynamically\n",
        "    run_name = f\"{args['model']}_b{args['batch_size']}_d{args['dropout']}\"\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=wandb.config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        args['batch_size'] = config.batch_size\n",
        "        args['dropout'] = config.dropout\n",
        "\n",
        "        # create dataset from data_list\n",
        "        dataloader = prepare_dataloader(data_list, args)\n",
        "\n",
        "        # train model\n",
        "        exp = TrainManager(dataloader, args)\n",
        "        exp.train(epochs=10, log=True, wb_log=True)\n"
      ],
      "metadata": {
        "id": "BinziYUTQwGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args['model'] = 'GROVER_FP'\n",
        "# run the sweep\n",
        "wandb.agent(sweep_id, run_sweep(data_list, args), count=4)"
      ],
      "metadata": {
        "id": "NxiWVecC85c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single run"
      ],
      "metadata": {
        "id": "Mc6kDkF_CyMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args['model'] = 'GROVER_FP'\n",
        "args['dropout'] = 0.2\n",
        "args['batch_size'] = 256\n",
        "args['hidden_channels'] = 256\n",
        "args['lr'] = 0.05\n",
        "\n",
        "run_name = f\"{args['model']}_b{args['batch_size']}_d{args['dropout']}_hdim{args['hidden_channels']}\"\n",
        "run = wandb.init(\n",
        "    name=run_name,\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"GDL_molecular_activity_prediction\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        'num_data_points': args['num_data_points'],\n",
        "        'assays': 'cell_based_high_hr',\n",
        "        'num_assays': args['num_assays'],\n",
        "\n",
        "        'model': args['model'],\n",
        "        'num_layers': args['num_layers'],\n",
        "        'hidden_channels': args['hidden_channels'],\n",
        "        'dropout': args['dropout'],\n",
        "        'batch_size': args['batch_size'],\n",
        "        'num_epochs': args['num_epochs'],\n",
        "        'lr': args['lr'],\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "LpDabvygfNUy",
        "outputId": "7af459f8-5a52-4714-ca90-f1fcffed355d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:s9910b3p) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC test</td><td>▂▁▂▁▂▂▅▄▅▄▆█▅▆▆▅▇▆▇▆██▇▆▅▇██▆▆▇▆▇▇▁▂▄▄█▄</td></tr><tr><td>AUC train</td><td>▂▁▂▁▂▂▄▄▄▃▅▇▅▆▆▅▇▆▇▇██▇▇▆███▆██▇██▁▂▄▄█▄</td></tr><tr><td>F1 test</td><td>▃▁▃▂▃▂▆▅▆▅▇█▇█▇▇█▇█▇███▇▇███▇▇█▇██▂▃▅▅▇▅</td></tr><tr><td>F1 train</td><td>▃▁▂▂▂▂▅▄▅▄▆▇▇▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█████▂▃▅▅▇▅</td></tr><tr><td>Precision test</td><td>▄▁▅▆▅█▅▅▅▇▅▅▆▅▅█▆▅▅▅▅▅▄▄▆▄▄▅▆▅▅▄▄▆▄▄▄▅▃▄</td></tr><tr><td>Precision train</td><td>▄▁▆▅▅█▆▅▆▇▇▅▇▇▆██▆▆▆▆▇▇▇▇▅▅▅▆▆▆▆▆▇▅▄▅▆▄▅</td></tr><tr><td>Recall test</td><td>▂▁▂▁▂▂▄▄▄▃▅█▅▆▅▅▆▆▆▆██▆▆▅▇██▅▆▆▆▇▇▁▂▃▄█▄</td></tr><tr><td>Recall train</td><td>▂▁▂▁▂▁▄▄▄▃▅▇▅▆▅▅▆▆▆▆██▇▆▅███▆▇▇▇██▁▂▄▄█▄</td></tr><tr><td>avg epoch time</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval time</td><td>▁▁▁█▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>██▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▂▅▅▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC test</td><td>0.55277</td></tr><tr><td>AUC train</td><td>0.5958</td></tr><tr><td>F1 test</td><td>0.12719</td></tr><tr><td>F1 train</td><td>0.19659</td></tr><tr><td>Precision test</td><td>0.14726</td></tr><tr><td>Precision train</td><td>0.19741</td></tr><tr><td>Recall test</td><td>0.13817</td></tr><tr><td>Recall train</td><td>0.21773</td></tr><tr><td>avg epoch time</td><td>16.14479</td></tr><tr><td>epoch</td><td>131</td></tr><tr><td>eval time</td><td>18.20111</td></tr><tr><td>loss</td><td>0.23462</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GROVER_FP_b256_d0.2_hdim256</strong> at: <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/s9910b3p' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/s9910b3p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230814_124813-s9910b3p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:s9910b3p). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230814_142119-ha8jl6ev</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/ha8jl6ev' target=\"_blank\">GROVER_FP_b256_d0.2_hdim256</a></strong> to <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/ha8jl6ev' target=\"_blank\">https://wandb.ai/davidkubanek/GDL_molecular_activity_prediction/runs/ha8jl6ev</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset from data_list\n",
        "dataloader = prepare_dataloader(data_list, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxCKJ7OjVhKG",
        "outputId": "88cffa3e-9bda-403a-a854-5e1632fc6718"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 194514\n",
            "Number of validation graphs: 64838\n",
            "Number of test graphs: 64839\n",
            "Example of a graph data object: Data(x=[29, 79], edge_index=[2, 62], edge_attr=[62, 10], y=[1, 5], fp=[2215], grover_fp=[5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "exp = TrainManager(dataloader, args)\n",
        "exp.train(epochs=100, log=True, wb_log=True)"
      ],
      "metadata": {
        "id": "QiLH8E9KVd1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "90a217ca-0b8f-454f-8766-fd511e6648ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [0/100]: 760it [00:16, 47.19it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.2589, Train AUC: 0.5000, Test AUC: 0.5000\n",
            "                        Train F1: 0.0001, Test F1: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/100]: 760it [00:16, 47.17it/s]                         \n",
            "Epoch [2/100]: 760it [00:16, 46.94it/s]\n",
            "Epoch [3/100]: 760it [00:16, 47.30it/s]\n",
            "Epoch [4/100]: 760it [00:16, 47.31it/s]\n",
            "Epoch [5/100]: 760it [00:15, 47.51it/s]\n",
            "Epoch [6/100]: 760it [00:16, 47.21it/s]\n",
            "Epoch [7/100]: 760it [00:16, 47.30it/s]\n",
            "Epoch [8/100]: 760it [00:16, 47.26it/s]\n",
            "Epoch [9/100]: 760it [00:15, 47.54it/s]\n",
            "Epoch [10/100]: 760it [00:16, 47.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Loss: 0.2381, Train AUC: 0.4995, Test AUC: 0.4993\n",
            "                        Train F1: 0.0029, Test F1: 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [11/100]: 760it [00:16, 47.30it/s]\n",
            "Epoch [12/100]: 760it [00:16, 47.26it/s]\n",
            "Epoch [13/100]: 760it [00:16, 47.30it/s]\n",
            "Epoch [14/100]: 760it [00:16, 47.17it/s]\n",
            "Epoch [15/100]: 760it [00:16, 47.28it/s]\n",
            "Epoch [16/100]: 760it [00:16, 47.42it/s]\n",
            "Epoch [17/100]: 760it [00:16, 47.37it/s]\n",
            "Epoch [18/100]: 760it [00:15, 47.51it/s]\n",
            "Epoch [19/100]: 760it [00:16, 47.46it/s]\n",
            "Epoch [20/100]: 760it [00:16, 47.08it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Loss: 0.3061, Train AUC: 0.4997, Test AUC: 0.4997\n",
            "                        Train F1: 0.0017, Test F1: 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [21/100]: 760it [00:16, 47.49it/s]\n",
            "Epoch [22/100]: 760it [00:15, 47.67it/s]\n",
            "Epoch [23/100]: 760it [00:16, 47.00it/s]                         \n",
            "Epoch [24/100]: 760it [00:16, 47.13it/s]\n",
            "Epoch [25/100]: 760it [00:16, 46.96it/s]                         \n",
            "Epoch [26/100]: 760it [00:16, 47.17it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6968b5adf6cf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8dadabdc11e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, log, wb_log)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8dadabdc11e6>\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training/test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     21\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     74\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# Collate attributes into a unified representation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             value, slices, incs = _collate(attr, values, data_list, stores,\n\u001b[0m\u001b[1;32m     86\u001b[0m                                            increment)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mincs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_incs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 values = [\n\u001b[0m\u001b[1;32m    137\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 values = [\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp.analyze()"
      ],
      "metadata": {
        "id": "IY-oKyawe1vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "yu38fmhbdGgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}